title: Algorithmic radicalization
id: 67058442
Algorithmic radicalization is the concept that recommender algorithms on popular social media sites such as YouTube and Facebook drive users toward progressively more extreme content over time leading to them developing radicalized extremist political views Algorithms record user interactions from likes dislikes to amount of time spent on posts to generate endless media aimed to keep users engaged Through echo chamber channels the consumer is driven to be more polarized through preferences in media and self confirmation br Algorithmic radicalization remains a controversial phenomenon as it is often not in the best interest of social media companies to remove echo chamber channels Though social media companies have admitted to algorithmic radicalization s existence it remains unclear how each will manage this growing threat br br br Social media echo chambers and filter bubbles br Social media platforms learn the interests and likes of the user to modify their experiences in their feed to keep them engaged and scrolling known as a filter bubble An echo chamber is formed when users come across beliefs that magnify or reinforce their thoughts and form a group of like minded users in a closed system Echo chambers spread information without any opposing beliefs and can possibly lead to confirmation bias According to group polarization theory an echo chamber can potentially lead users and groups towards more extreme radicalized positions According to the National Library of Medicine Users online tend to prefer information adhering to their worldviews ignore dissenting information and form polarized groups around shared narratives Furthermore when polarization is high misinformation quickly proliferates br br br By site br br br Facebook br br br Facebook s algorithms br Facebook s algorithm focuses on recommending content that makes the user want to interact They rank content by prioritizing popular posts by friends viral content and sometimes divisive content Each feed is personalized to the user s specific interests which can sometimes lead users towards an echo chamber of troublesome content Users can find their list of interests the algorithm uses by going to the Your ad Preferences page According to a Pew Research study of Facebook users did not know that list existed until they were directed towards that page in the study It is also relatively common for Facebook to assign political labels to their users In recent years Facebook has started using artificial intelligence to change the content users see in their feed and what is recommended to them A document known as The Facebook Files has revealed that their AI system prioritizes user engagement over everything else The Facebook Files has also demonstrated that controlling the AI systems has proven difficult to handle br br br Facebook s allegations br In an August internal memo leaked in Facebook has admitted that the mechanics of our platforms are not neutral concluding that in order to reach maximum profits optimization for engagement is necessary In order to increase engagement algorithms have found that hate misinformation and politics are instrumental for app activity As referenced in the memo The more incendiary the material the more it keeps users engaged the more it is boosted by the algorithm According to a study false rumors spread faster and wider than true information They found falsehoods are more likely to be retweeted on Twitter than the truth and reach their first people six times faster This effect is more pronounced with political news than other categories br br br YouTube br br br YouTube s algorithm br YouTube has been around since and has more than billion monthly users YouTube discovery content systems focus on the user s personal activity watched favorites likes to direct them to recommended content YouTube s algorithm is accountable for roughly of users recommended videos and what drives people to watch certain content According to a new study users have little power to keep unsolicited videos out of their suggested recommended content This includes videos about hate speech livestreams etc br br br YouTube s allegations br YouTube has been identified as an influential platform for spreading radicalized content Al Qaeda and similar extremist groups have been linked to using YouTube for recruitment videos and engaging with international media outlets In a research study published by the American Behavioral Scientist Journal they researched whether it is possible to identify a set of attributes that may help explain part of the YouTube algorithm s decision making process The results of the study showed that YouTube s algorithm recommendations for extremism content factor into the presence of radical keywords in a video s title In February in the case of Gonzalez v Google the question at hand is whether or not Google the parent company of YouTube is protected from lawsuits claiming that the site s algorithms aided terrorists in recommending ISIS videos to users Section is known to generally protect online platforms from civil liability for the content posted by its users br br br TikTok br br br TikTok algorithms br TikTok is an app that recommends videos to a user s For You Page FYP making every users page different With the nature of the algorithm behind the app TikTok s FYP has been linked to showing more explicit and radical videos over time based on users previous interactions on the app Since TikTok s inception the app has been scrutinized for misinformation and hate speech as those forms of media usually generate more interactions to the algorithm br As of TikTok s head of US Security has put out a statement that videos were removed globally between April June for violating our Community Guidelines or Terms of Service to cut back on hate speech harassment and misinformation br br br Alt right pipeline br br br Self radicalization br br The U S Department of Justice defines Lone wolf self terrorism as someone who acts alone in a terrorist attack without the help or encouragement of a government or a terrorist organization Through social media outlets on the internet Lone wolf terrorism has been on the rise being linked to algorithmic radicalization Through echo chambers on the internet viewpoints typically seen as radical were accepted and quickly adopted by other extremists These viewpoints are encouraged by forums group chats and social media to reinforce their beliefs br br br References in media br br br The Social Dilemma br The Social Dilemma is a docudrama about how algorithms behind social media enables addiction while possessing abilities to manipulate people s views emotions and behavior to spread conspiracy theories and disinformation The film repeatedly uses buzz words such as echo chambers and fake news to prove psychological manipulation on social media therefore leading to political manipulation In the film Ben falls deeper into a social media addiction as the algorithm found that his social media page has a chance of long term engagement This leads into more videos on the recommended feed for Ben and he eventually becomes more immersed into propaganda and conspiracy theories becoming more polarized with each video br br br Proposed solutions br br br Weakening Section protections br br In the Communications Decency Act Section states that No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider Section protects the media from liabilities or being sued of third party content such as illegal activity from a user However critics argue that this approach reduces a company s incentive to remove harmful content or misinformation and this loophole has allowed social media companies to maximize profits through pushing radical content without legal risks This claim has itself been criticized by proponents of Section who believe that the Good Samaritan clause in subsection c enables websites to moderate in the first place and that prior to its passing courts had ruled in Stratton Oakmont Inc v Prodigy Services Co that moderation in any capacity introduces a liability to content providers as publishers of the content they chose to leave up br Lawmakers have drafted legislation that would weaken or remove Section protections over algorithmic content House Democrats Anna Eshoo Frank Pallone Jr Mike Doyle and Jan Schakowsky introduced the Justice Against Malicious Algorithms Act in October as H R The bill died in committee but it would have removed Section protections for service providers related to personalized recommendation algorithms that present content to users if those algorithms knowingly or recklessly deliver content that contributes to physical or severe emotional injury br br br See also br br br 