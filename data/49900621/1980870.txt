title: Benchmark (computing)
id: 1980870
In computing a benchmark is the act of running a computer program a set of programs or other operations in order to assess the relative performance of an object normally by running a number of standard tests and trials against it br The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves br Benchmarking is usually associated with assessing performance characteristics of computer hardware for example the floating point operation performance of a CPU but there are circumstances when the technique is also applicable to software Software benchmarks are for example run against compilers or database management systems DBMS br Benchmarks provide a method of comparing the performance of various subsystems across different chip system architectures Benchmarking as a part of continuous integration is called Continuous Benchmarking br br As computer architecture advanced it became more difficult to compare the performance of various computer systems simply by looking at their specifications Therefore tests were developed that allowed comparison of different architectures For example Pentium processors generally operated at a higher clock frequency than Athlon XP or PowerPC processors which did not necessarily translate to more computational power a processor with a slower clock frequency might perform as well as or even better than a processor operating at a higher frequency See BogoMips and the megahertz myth br Benchmarks are designed to mimic a particular type of workload on a component or system Synthetic benchmarks do this by specially created programs that impose the workload on the component Application benchmarks run real world programs on the system While application benchmarks usually give a much better measure of real world performance on a given system synthetic benchmarks are useful for testing individual components like a hard disk or networking device br Benchmarks are particularly important in CPU design giving processor architects the ability to measure and make tradeoffs in microarchitectural decisions For example if a benchmark extracts the key algorithms of an application it will contain the performance sensitive aspects of that application Running this much smaller snippet on a cycle accurate simulator can give clues on how to improve performance br Prior to computer and microprocessor architects used SPEC to do this although SPEC s Unix based benchmarks were quite lengthy and thus unwieldy to use intact br Computer manufacturers are known to configure their systems to give unrealistically high performance on benchmark tests that are not replicated in real usage For instance during the s some compilers could detect a specific mathematical operation used in a well known floating point benchmark and replace the operation with a faster mathematically equivalent operation However such a transformation was rarely useful outside the benchmark until the mid s when RISC and VLIW architectures emphasized the importance of compiler technology as it related to performance Benchmarks are now regularly used by compiler companies to improve not only their own benchmark scores but real application performance br CPUs that have many execution units such as a superscalar CPU a VLIW CPU or a reconfigurable computing CPU typically have slower clock rates than a sequential CPU with one or two execution units when built from transistors that are just as fast Nevertheless CPUs with many execution units often complete real world and benchmark tasks in less time than the supposedly faster high clock rate CPU br Given the large number of benchmarks available a manufacturer can usually find at least one benchmark that shows its system will outperform another system the other systems can be shown to excel with a different benchmark br Manufacturers commonly report only those benchmarks or aspects of benchmarks that show their products in the best light They also have been known to mis represent the significance of benchmarks again to show their products in the best possible light Taken together these practices are called bench marketing br Ideally benchmarks should only substitute for real applications if the application is unavailable or too difficult or costly to port to a specific processor or computer system If performance is critical the only benchmark that matters is the target environment s application suite br br Features of benchmarking software may include recording exporting the course of performance to a spreadsheet file visualization such as drawing line graphs or color coded tiles and pausing the process to be able to resume without having to start over Software can have additional features specific to its purpose for example disk benchmarking software may be able to optionally start measuring the disk speed within a specified range of the disk rather than the full disk measure random access reading speed and latency have a quick scan feature which measures the speed through samples of specified intervals and sizes and allow specifying a data block size meaning the number of requested bytes per read request br br Benchmarking is not easy and often involves several iterative rounds in order to arrive at predictable useful conclusions Interpretation of benchmarking data is also extraordinarily difficult Here is a partial list of common challenges br br Vendors tend to tune their products specifically for industry standard benchmarks Norton SysInfo SI is particularly easy to tune for since it mainly biased toward the speed of multiple operations Use extreme caution in interpreting such results br Some vendors have been accused of cheating at benchmarks doing things that give much higher benchmark numbers but make things worse on the actual likely workload br Many benchmarks focus entirely on the speed of computational performance neglecting other important features of a computer system such as br Qualities of service aside from raw performance Examples of unmeasured qualities of service include security availability reliability execution integrity serviceability scalability especially the ability to quickly and nondisruptively add or reallocate capacity etc There are often real trade offs between and among these qualities of service and all are important in business computing Transaction Processing Performance Council Benchmark specifications partially address these concerns by specifying ACID property tests database scalability rules and service level requirements br In general benchmarks do not measure Total cost of ownership Transaction Processing Performance Council Benchmark specifications partially address this concern by specifying that a price performance metric must be reported in addition to a raw performance metric using a simplified TCO formula However the costs are necessarily only partial and vendors have been known to price specifically and only for the benchmark designing a highly specific benchmark special configuration with an artificially low price Even a tiny deviation from the benchmark package results in a much higher price in real world experience br Facilities burden space power and cooling When more power is used a portable system will have a shorter battery life and require recharging more often A server that consumes more power and or space may not be able to fit within existing data center resource constraints including cooling limitations There are real trade offs as most semiconductors require more power to switch faster See also performance per watt br In some embedded systems where memory is a significant cost better code density can significantly reduce costs br Vendor benchmarks tend to ignore requirements for development test and disaster recovery computing capacity Vendors only like to report what might be narrowly required for production capacity in order to make their initial acquisition price seem as low as possible br Benchmarks are having trouble adapting to widely distributed servers particularly those with extra sensitivity to network topologies The emergence of grid computing in particular complicates benchmarking since some workloads are grid friendly while others are not br Users can have very different perceptions of performance than benchmarks may suggest In particular users appreciate predictability servers that always meet or exceed service level agreements Benchmarks tend to emphasize mean scores IT perspective rather than maximum worst case response times real time computing perspective or low standard deviations user perspective br Many server architectures degrade dramatically at high near levels of usage fall off a cliff and benchmarks should but often do not take that factor into account Vendors in particular tend to publish server benchmarks at continuous at about usage an unrealistic situation and do not document what happens to the overall system when demand spikes beyond that level br Many benchmarks focus on one application or even one application tier to the exclusion of other applications Most data centers are now implementing virtualization extensively for a variety of reasons and benchmarking is still catching up to that reality where multiple applications and application tiers are concurrently running on consolidated servers br There are few if any high quality benchmarks that help measure the performance of batch computing especially high volume concurrent batch and online computing Batch computing tends to be much more focused on the predictability of completing long running tasks correctly before deadlines such as end of month or end of fiscal year Many important core business processes are batch oriented and probably always will be such as billing br Benchmarking institutions often disregard or do not follow basic scientific method This includes but is not limited to small sample size lack of variable control and the limited repeatability of results br br There are seven vital characteristics for benchmarks These key properties are br br Relevance Benchmarks should measure relatively vital features br Representativeness Benchmark performance metrics should be broadly accepted by industry and academia br Equity All systems should be fairly compared br Repeatability Benchmark results can be verified br Cost effectiveness Benchmark tests are economical br Scalability Benchmark tests should work across systems possessing a range of resources from low to high br Transparency Benchmark metrics should be easy to understand br br Real program br word processing software br tool software of CAD br user s application software i e MIS br Video games br Compilers building a large project for example Chromium browser or Linux kernel br Component Benchmark Microbenchmark br core routine consists of a relatively small and specific piece of code br measure performance of a computer s basic components br may be used for automatic detection of computer s hardware parameters like number of registers cache size memory latency etc br Kernel br contains key codes br normally abstracted from actual program br popular kernel Livermore loop br linpack benchmark contains basic linear algebra subroutine written in FORTRAN language br results are represented in Mflop s br Synthetic Benchmark br Procedure for programming synthetic benchmark br take statistics of all types of operations from many application programs br get proportion of each operation br write program based on the proportion above br Types of Synthetic Benchmark are br Whetstone br Dhrystone br These were the first general purpose industry standard computer benchmarks They do not necessarily obtain high scores on modern pipelined computers br I O benchmarks br Database benchmarks br measure the throughput and response times of database management systems DBMS br Parallel benchmarks br used on machines with multiple cores and or processors or systems consisting of multiple machines br br Business Applications Performance Corporation BAPCo br Embedded Microprocessor Benchmark Consortium EEMBC br Standard Performance Evaluation Corporation SPEC in particular their SPECint and SPECfp br Transaction Processing Performance Council TPC DBMS benchmarks br br AIM Multiuser Benchmark composed of a list of tests that could be mixed to create a load mix that would simulate a specific computer function on any UNIX type OS br Bonnie filesystem and hard drive benchmark br BRL CAD cross platform architecture agnostic benchmark suite based on multithreaded ray tracing performance baselined against a VAX and used since for evaluating relative CPU performance compiler differences optimization levels coherency architecture differences and operating system differences br Collective Knowledge customizable cross platform framework to crowdsource benchmarking and optimization of user workloads such as deep learning across hardware provided by volunteers br Coremark Embedded computing benchmark br DEISA Benchmark Suite scientific HPC applications benchmark br Dhrystone integer arithmetic performance often reported in DMIPS Dhrystone millions of instructions per second br DiskSpd Command line tool for storage benchmarking that generates a variety of requests against computer files partitions or storage devices br Fhourstones an integer benchmark br HINT designed to measure overall CPU and memory performance br Iometer I O subsystem measurement and characterization tool for single and clustered systems br IOzone Filesystem benchmark br LINPACK benchmarks traditionally used to measure FLOPS br Livermore loops br NAS parallel benchmarks br NBench synthetic benchmark suite measuring performance of integer arithmetic memory operations and floating point arithmetic br PAL a benchmark for realtime physics engines br PerfKitBenchmarker A set of benchmarks to measure and compare cloud offerings br Phoronix Test Suite open source cross platform benchmarking suite for Linux OpenSolaris FreeBSD OSX and Windows It includes a number of other benchmarks included on this page to simplify execution br POV Ray D render br Tak function a simple benchmark used to test recursion performance br TATP Benchmark Telecommunication Application Transaction Processing Benchmark br TPoX An XML transaction processing benchmark for XML databases br VUP VAX unit of performance also called VAX MIPS br Whetstone floating point arithmetic performance often reported in millions of Whetstone instructions per second MWIPS br br BAPCo MobileMark SYSmark WebMark br CrystalDiskMark br Underwriters Laboratories UL DMark PCMark br Heaven Benchmark br PiFast br Superposition Benchmark br Super PI br SuperPrime br Valley Benchmark br Whetstone br Windows System Assessment Tool included with Windows Vista and later releases providing an index for consumers to rate their systems easily br Worldbench discontinued br br AnTuTu commonly used on phones and ARM based devices br Byte Sieve originally tested language performance but widely used as a machine benchmark as well br Creative Computing Benchmark Compares the BASIC programming language on various platforms Introduced in br Geekbench A cross platform benchmark for Windows Linux macOS iOS and Android br iCOMP the Intel comparative microprocessor performance published by Intel br Khornerstone br Performance Rating modeling scheme used by AMD and Cyrix to reflect the relative performance usually compared to competing products br Rugg Feldman benchmarks one of the earliest microcomputer benchmarks from br SunSpider a browser speed test br VMmark a virtualization benchmark suite br br Benchmarking business perspective br Figure of merit br Lossless compression benchmarks br Performance Counter Monitor br Test suite a collection of test cases intended to show that a software program has some specified set of behaviors br br Gray Jim ed The Benchmark Handbook for Database and Transaction Systems Morgan Kaufmann Series in Data Management Systems nd ed Morgan Kaufmann Publishers Inc ISBN br Scalzo Bert Kline Kevin Fernandez Claudia Burleson Donald K Ault Mike Database Benchmarking Practical Methods for Oracle SQL Server Rampant TechPress ISBN br Nambiar Raghunath Poess Meikel eds Performance Evaluation and Benchmarking Springer ISBN br br Lewis Byron C Crews Albert E The Evolution of Benchmarking as a Computer Performance Evaluation Technique MIS Quarterly doi ISSN JSTOR The dates 