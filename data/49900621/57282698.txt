title: ZFS
id: 57282698
ZFS previously Zettabyte File System is a file system with volume management capabilities It began as part of the Sun Microsystems Solaris operating system in Large parts of Solaris including ZFS were published under an open source license as OpenSolaris for around years from before being placed under a closed source license when Oracle Corporation acquired Sun in During to the open source version of ZFS was ported to Linux Mac OS X continued as MacZFS and FreeBSD In the illumos project forked a recent version of OpenSolaris including ZFS to continue its development as an open source project In OpenZFS was founded to coordinate the development of open source ZFS OpenZFS maintains and manages the core ZFS code while organizations using ZFS maintain the specific code and validation processes required for ZFS to integrate within their systems OpenZFS is widely used in Unix like systems br br br Overview br The management of stored data generally involves two aspects the physical volume management of one or more block storage devices such as hard drives and SD cards including their organization into logical block devices as VDEVs ZFS Virtual Device as seen by the operating system often involving a volume manager RAID controller array manager or suitable device driver and the management of data and files that are stored on these logical block devices a file system or other data storage br br Example A RAID array of hard drives and an SSD caching disk is controlled by Intel s RST system part of the chipset and firmware built into a desktop computer The Windows user sees this as a single volume containing an NTFS formatted drive of their data and NTFS is not necessarily aware of the manipulations that may be required such as reading from writing to the cache drive or rebuilding the RAID array if a disk fails The management of the individual devices and their presentation as a single device is distinct from the management of the files held on that apparent device br ZFS is unusual because unlike most other storage systems it unifies both of these roles and acts as both the volume manager and the file system Therefore it has complete knowledge of both the physical disks and volumes including their status condition and logical arrangement into volumes as well as of all the files stored on them ZFS is designed to ensure subject to suitable hardware that data stored on disks cannot be lost due to physical errors misprocessing by the hardware or operating system or bit rot events and data corruption that may happen over time Its complete control of the storage system is used to ensure that every step whether related to file management or disk management is verified confirmed corrected if needed and optimized in a way that the storage controller cards and separate volume and file managers cannot achieve br ZFS also includes a mechanism for dataset and pool level snapshots and replication including snapshot cloning which is described by the FreeBSD documentation as one of its most powerful features with functionality that even other file systems with snapshot functionality lack Very large numbers of snapshots can be taken without degrading performance allowing snapshots to be used prior to risky system operations and software changes or an entire production live file system to be fully snapshotted several times an hour in order to mitigate data loss due to user error or malicious activity Snapshots can be rolled back live or previous file system states can be viewed even on very large file systems leading to savings in comparison to formal backup and restore processes Snapshots can also be cloned to form new independent file systems ZFS also has the ability to take a pool level snapshot known as a checkpoint which allows rollback of operations that may affect the entire pool s structure or that add or remove entire datasets br br br History br br br Development at Sun Microsystems br In AT T Corporation and Sun announced that they were collaborating on a project to merge the most popular Unix variants on the market at that time Berkeley Software Distribution UNIX System V and Xenix This became Unix System V Release SVR The project was released under the name Solaris which became the successor to SunOS although SunOS x micro releases were retroactively named Solaris br ZFS was designed and implemented by a team at Sun led by Jeff Bonwick Bill Moore and Matthew Ahrens It was announced on September but development started in Source code for ZFS was integrated into the main trunk of Solaris development on October and released for developers as part of build of OpenSolaris on November In June Sun announced that ZFS was included in the mainstream update to Solaris br Solaris was originally developed as proprietary software but Sun Microsystems was an early commercial proponent of open source software and in June released most of the Solaris codebase under the CDDL license and founded the OpenSolaris open source project In Solaris U Sun added the ZFS file system and frequently updated ZFS with new features during the next years ZFS was ported to Linux Mac OS X continued as MacZFS and FreeBSD under this open source license br The name at one point was said to stand for Zettabyte File System but by the name was no longer considered to be an abbreviation A ZFS file system can store up to quadrillion zettabytes ZB br In September NetApp sued Sun claiming that ZFS infringed some of NetApp s patents on Write Anywhere File Layout Sun counter sued in October the same year claiming the opposite The lawsuits were ended in with an undisclosed settlement br br br current Development at Oracle OpenZFS br br Ported versions of ZFS began to appear in After the Sun acquisition by Oracle in Oracle s version of ZFS became closed source and the development of open source versions proceeded independently coordinated by OpenZFS from br br br Features br br br Summary br Examples of features specific to ZFS include br br Designed for long term storage of data and indefinitely scaled datastore sizes with zero data loss and high configurability br Hierarchical checksumming of all data and metadata ensuring that the entire storage system can be verified on use and confirmed to be correctly stored or remedied if corrupt Checksums are stored with a block s parent block rather than with the block itself This contrasts with many file systems where checksums if held are stored with the data so that if the data is lost or corrupt the checksum is also likely to be lost or incorrect br Can store a user specified number of copies of data or metadata or selected types of data to improve the ability to recover from data corruption of important files and structures br Automatic rollback of recent changes to the file system and data in some circumstances in the event of an error or inconsistency br Automated and usually silent self healing of data inconsistencies and write failure when detected for all errors where the data is capable of reconstruction Data can be reconstructed using all of the following error detection and correction checksums stored in each block s parent block multiple copies of data including checksums held on the disk write intentions logged on the SLOG ZIL for writes that should have occurred but did not occur after a power failure parity data from RAID RAID Z disks and volumes copies of data from mirrored disks and volumes br Native handling of standard RAID levels and additional ZFS RAID layouts RAID Z The RAID Z levels stripe data across only the disks required for efficiency many RAID systems stripe indiscriminately across all devices and checksumming allows rebuilding of inconsistent or corrupted data to be minimized to those blocks with defects br Native handling of tiered storage and caching devices which is usually a volume related task Because ZFS also understands the file system it can use file related knowledge to inform integrate and optimize its tiered storage handling which a separate device cannot br Native handling of snapshots and backup replication which can be made efficient by integrating the volume and file handling Relevant tools are provided at a low level and require external scripts and software for utilization br Native data compression and deduplication although the latter is largely handled in RAM and is memory hungry br Efficient rebuilding of RAID arrays a RAID controller often has to rebuild an entire disk but ZFS can combine disk and file knowledge to limit any rebuilding to data which is actually missing or corrupt greatly speeding up rebuilding br Unaffected by RAID hardware changes which affect many other systems On many systems if self contained RAID hardware such as a RAID card fails or the data is moved to another RAID system the file system will lack information that was on the original RAID hardware which is needed to manage data on the RAID array This can lead to a total loss of data unless near identical hardware can be acquired and used as a stepping stone Since ZFS manages RAID itself a ZFS pool can be migrated to other hardware or the operating system can be reinstalled and the RAID Z structures and data will be recognized and immediately accessible by ZFS again br Ability to identify data that would have been found in a cache but has been discarded recently instead this allows ZFS to reassess its caching decisions in light of later use and facilitates very high cache hit levels ZFS cache hit rates are typically over br Alternative caching strategies can be used for data that would otherwise cause delays in data handling For example synchronous writes which are capable of slowing down the storage system can be converted to asynchronous writes by being written to a fast separate caching device known as the SLOG sometimes called the ZIL ZFS Intent Log br Highly tunable many internal parameters can be configured for optimal functionality br Can be used for high availability clusters and computing although not fully designed for this use br br br Data integrity br br One major feature that distinguishes ZFS from other file systems is that it is designed with a focus on data integrity by protecting the user s data on disk against silent data corruption caused by data degradation power surges voltage spikes bugs in disk firmware phantom writes the previous write did not make it to disk misdirected reads writes the disk accesses the wrong block DMA parity errors between the array and server memory or from the driver since the checksum validates data inside the array driver errors data winds up in the wrong buffer inside the kernel accidental overwrites such as swapping to a live file system etc br A study showed that neither any of the then major and widespread filesystems such as UFS Ext XFS JFS or NTFS nor hardware RAID which has some issues with data integrity provided sufficient protection against data corruption problems Initial research indicates that ZFS protects data better than earlier efforts br It is also faster than UFS and can be seen as its replacement br Within ZFS data integrity is achieved by using a Fletcher based checksum or a SHA hash throughout the file system tree Each block of data is checksummed and the checksum value is then saved in the pointer to that block rather than at the actual block itself Next the block pointer is checksummed with the value being saved at its pointer This checksumming continues all the way up the file system s data hierarchy to the root node which is also checksummed thus creating a Merkle tree In flight data corruption or phantom reads writes the data written read checksums correctly but is actually wrong are undetectable by most filesystems as they store the checksum with the data ZFS stores the checksum of each block in its parent block pointer so that the entire pool self validates br When a block is accessed regardless of whether it is data or meta data its checksum is calculated and compared with the stored checksum value of what it should be If the checksums match the data are passed up the programming stack to the process that asked for it if the values do not match then ZFS can heal the data if the storage pool provides data redundancy such as with internal mirroring assuming that the copy of data is undamaged and with matching checksums It is optionally possible to provide additional in pool redundancy by specifying copies or copies which means that data will be stored twice or three times on the disk effectively halving or for copies reducing to one third the storage capacity of the disk Additionally some kinds of data used by ZFS to manage the pool are stored multiple times by default for safety even with the default copies setting br If other copies of the damaged data exist or can be reconstructed from checksums and parity data ZFS will use a copy of the data or recreate it via a RAID recovery mechanism and recalculate the checksum ideally resulting in the reproduction of the originally expected value If the data passes this integrity check the system can then update all faulty copies with known good data and redundancy will be restored br If there are no copies of the damaged data ZFS puts the pool in a faulted state preventing its future use and providing no documented ways to recover pool contents br Consistency of data held in memory such as cached data in the ARC is not checked by default as ZFS is expected to run on enterprise quality hardware with error correcting RAM However the capability to check in memory data exists and can be enabled using debug flags br br br RAID RAID Z br For ZFS to be able to guarantee data integrity it needs multiple copies of the data usually spread across multiple disks This is typically achieved by using either a RAID controller or so called soft RAID built into a file system br br br Avoidance of hardware RAID controllers br While ZFS can work with hardware RAID devices it will usually work more efficiently and with greater data protection if it has raw access to all storage devices ZFS relies on the disk for an honest view to determine the moment data is confirmed as safely written and has numerous algorithms designed to optimize its use of caching cache flushing and disk handling br Disks connected to the system using a hardware firmware other soft RAID or any other controller that modifies the ZFS to disk I O path will affect ZFS performance and data integrity If a third party device performs caching or presents drives to ZFS as a single system without the low level view ZFS relies upon there is a much greater chance that the system will perform less optimally and that ZFS will be less likely to prevent failures recover from failures more slowly or lose data due to a write failure For example if a hardware RAID card is used ZFS may not be able to determine the condition of disks determine if the RAID array is degraded or rebuilding detect all data corruption place data optimally across the disks make selective repairs control how repairs are balanced with ongoing use or make repairs that ZFS could usually undertake The hardware RAID card will interfere with ZFS algorithms RAID controllers also usually add controller dependent data to the drives which prevents software RAID from accessing the user data In the case of a hardware RAID controller failure it may be possible to read the data with another compatible controller but this isn t always possible and a replacement may not be available Alternate hardware RAID controllers may not understand the original manufacturer s custom data required to manage and restore an array br Unlike most other systems where RAID cards or similar hardware can offload resources and processing to enhance performance and reliability with ZFS it is strongly recommended that these methods not be used as they typically reduce the system s performance and reliability br If disks must be attached through a RAID or other controller it is recommended to minimize the amount of processing done in the controller by using a plain HBA host adapter a simple fanout card or configure the card in JBOD mode i e turn off RAID and caching functions to allow devices to be attached with minimal changes in the ZFS to disk I O pathway A RAID card in JBOD mode may still interfere if it has a cache or depending upon its design may detach drives that do not respond in time as has been seen with many energy efficient consumer grade hard drives and as such may require Time Limited Error Recovery TLER CCTL ERC enabled drives to prevent drive dropouts so not all cards are suitable even with RAID functions disabled br br br ZFS s approach RAID Z and mirroring br Instead of hardware RAID ZFS employs soft RAID offering RAID Z parity based like RAID and similar and disk mirroring similar to RAID The schemes are highly flexible br RAID Z is a data parity distribution scheme like RAID but uses dynamic stripe width every block is its own RAID stripe regardless of blocksize resulting in every RAID Z write being a full stripe write This when combined with the copy on write transactional semantics of ZFS eliminates the write hole error RAID Z is also faster than traditional RAID because it does not need to perform the usual read modify write sequence br As all stripes are of different sizes RAID Z reconstruction has to traverse the filesystem metadata to determine the actual RAID Z geometry This would be impossible if the filesystem and the RAID array were separate products whereas it becomes feasible when there is an integrated view of the logical and physical structure of the data Going through the metadata means that ZFS can validate every block against its bit checksum as it goes whereas traditional RAID products usually cannot do this br In addition to handling whole disk failures RAID Z can also detect and correct silent data corruption offering self healing data when reading a RAID Z block ZFS compares it against its checksum and if the data disks did not return the right answer ZFS reads the parity and then figures out which disk returned bad data Then it repairs the damaged data and returns good data to the requestor br RAID Z and mirroring do not require any special hardware they do not need NVRAM for reliability and they do not need write buffering for good performance or data protection With RAID Z ZFS provides fast reliable storage using cheap commodity disks br There are five different RAID Z modes striping similar to RAID offers no redundancy RAID Z similar to RAID allows one disk to fail RAID Z similar to RAID allows two disks to fail RAID Z a RAID configuration allows three disks to fail and mirroring similar to RAID allows all but one disk to fail br The need for RAID Z arose in the early s as multi terabyte capacity drives became more common This increase in capacity without a corresponding increase in throughput speeds meant that rebuilding an array due to a failed drive could easily take weeks or months to complete During this time the older disks in the array will be stressed by the additional workload which could result in data corruption or drive failure By increasing parity RAID Z reduces the chance of data loss by simply increasing redundancy br br br Resilvering and scrub array syncing and integrity checking br ZFS has no tool equivalent to fsck the standard Unix and Linux data checking and repair tool for file systems Instead ZFS has a built in scrub function which regularly examines all data and repairs silent corruption and other problems Some differences are br br fsck must be run on an offline filesystem which means the filesystem must be unmounted and is not usable while being repaired while scrub is designed to be used on a mounted live filesystem and does not need the ZFS filesystem to be taken offline br fsck usually only checks metadata such as the journal log but never checks the data itself This means after an fsck the data might still not match the original data as stored br fsck cannot always validate and repair data when checksums are stored with data often the case in many file systems because the checksums may also be corrupted or unreadable ZFS always stores checksums separately from the data they verify improving reliability and the ability of scrub to repair the volume ZFS also stores multiple copies of data metadata in particular may have upwards of or copies multiple copies per disk and multiple disk mirrors per volume greatly improving the ability of scrub to detect and repair extensive damage to the volume compared to fsck br scrub checks everything including metadata and the data The effect can be observed by comparing fsck to scrub times sometimes a fsck on a large RAID completes in a few minutes which means only the metadata was checked Traversing all metadata and data on a large RAID takes many hours which is exactly what scrub does br while fsck detects and tries to fix errors using available filesystem data scrub relies on redundancy to recover from issues While fsck offers to fix the file system with partial data loss scrub puts it into faulted state if there is no redundancy br The official recommendation from Sun Oracle is to scrub enterprise level disks once a month and cheaper commodity disks once a week br br br Capacity br ZFS is a bit file system so it can address times more data than bit systems such as Btrfs The maximum limits of ZFS are designed to be so large that they should never be encountered in practice For instance fully populating a single zpool with bits of data would require TB hard disk drives br Some theoretical limits in ZFS are br br exbibytes bytes maximum size of a single file br number of entries in any individual directory br exbibytes maximum size of any attribute br number of attributes of a file actually constrained to for the number of files in a directory br quadrillion zebibytes bytes maximum size of any zpool br number of devices in any zpool br number of file systems in a zpool br number of zpools in a system br br br Encryption br With Oracle Solaris the encryption capability in ZFS is embedded into the I O pipeline During writes a block may be compressed encrypted checksummed and then deduplicated in that order The policy for encryption is set at the dataset level when datasets file systems or ZVOLs are created The wrapping keys provided by the user administrator can be changed at any time without taking the file system offline The default behaviour is for the wrapping key to be inherited by any child data sets The data encryption keys are randomly generated at dataset creation time Only descendant datasets snapshots and clones share data encryption keys A command to switch to a new data encryption key for the clone or at any time is provided this does not re encrypt already existing data instead utilising an encrypted master key mechanism br As of the encryption feature is also fully integrated into OpenZFS available for Debian and Ubuntu Linux distributions br br br Read write efficiency br ZFS will automatically allocate data storage across all vdevs in a pool and all devices in each vdev in a way that generally maximises the performance of the pool ZFS will also update its write strategy to take account of new disks added to a pool when they are added br As a general rule ZFS allocates writes across vdevs based on the free space in each vdev This ensures that vdevs which have proportionately less data already are given more writes when new data is to be stored This helps to ensure that as the pool becomes more used the situation does not develop that some vdevs become full forcing writes to occur on a limited number of devices It also means that when data is read and reads are much more frequent than writes in most uses different parts of the data can be read from as many disks as possible at the same time giving much higher read performance Therefore as a general rule pools and vdevs should be managed and new storage added so that the situation does not arise that some vdevs in a pool are almost full and others almost empty as this will make the pool less efficient br br br Other features br br br Storage devices spares and quotas br Pools can have hot spares to compensate for failing disks When mirroring block devices can be grouped according to physical chassis so that the filesystem can continue in the case of the failure of an entire chassis br Storage pool composition is not limited to similar devices but can consist of ad hoc heterogeneous collections of devices which ZFS seamlessly pools together subsequently doling out space to datasets file system instances or ZVOLs as needed Arbitrary storage device types can be added to existing pools to expand their size br The storage capacity of all vdevs is available to all of the file system instances in the zpool A quota can be set to limit the amount of space a file system instance can occupy and a reservation can be set to guarantee that space will be available to a file system instance br br br Caching mechanisms ARC L ARC Transaction groups ZIL SLOG Special VDEV br ZFS uses different layers of disk cache to speed up read and write operations Ideally all data should be stored in RAM but that is usually too expensive Therefore data is automatically cached in a hierarchy to optimize performance versus cost these are often called hybrid storage pools Frequently accessed data will be stored in RAM and less frequently accessed data can be stored on slower media such as solid state drives SSDs Data that is not often accessed is not cached and left on the slow hard drives If old data is suddenly read a lot ZFS will automatically move it to SSDs or to RAM br ZFS caching mechanisms include one each for reads and writes and in each case two levels of caching can exist one in computer memory RAM and one on fast storage usually solid state drives SSDs for a total of four caches br br A number of other caches cache divisions and queues also exist within ZFS For example each VDEV has its own data cache and the ARC cache is divided between data stored by the user and metadata used by ZFS with control over the balance between these br br br Special VDEV Class br In OpenZFS and later it is possible to configure a Special VDEV class to preferentially store filesystem metadata and optionally the Data Deduplication Table DDT and small filesystem blocks This allows for example to create a Special VDEV on fast solid state storage to store the metadata while the regular file data is stored on spinning disks This speeds up metadata intensive operations such as filesystem traversal scrub and resilver without the expense of storing the entire filesystem on solid state storage br br br Copy on write transactional model br ZFS uses a copy on write transactional object model All block pointers within the filesystem contain a bit checksum or bit hash currently a choice between Fletcher Fletcher or SHA of the target block which is verified when the block is read Blocks containing active data are never overwritten in place instead a new block is allocated modified data is written to it then any metadata blocks referencing it are similarly read reallocated and written To reduce the overhead of this process multiple updates are grouped into transaction groups and ZIL intent log write cache is used when synchronous write semantics are required The blocks are arranged in a tree as are their checksums see Merkle signature scheme br br br Snapshots and clones br br An advantage of copy on write is that when ZFS writes new data the blocks containing the old data can be retained allowing a snapshot version of the file system to be maintained ZFS snapshots are consistent they reflect the entire data as it existed at a single point in time and can be created extremely quickly since all the data composing the snapshot is already stored with the entire storage pool often snapshotted several times per hour They are also space efficient since any unchanged data is shared among the file system and its snapshots Snapshots are inherently read only ensuring they will not be modified after creation although they should not be relied on as a sole means of backup Entire snapshots can be restored and also files and directories within snapshots br Writeable snapshots clones can also be created resulting in two independent file systems that share a set of blocks As changes are made to any of the clone file systems new data blocks are created to reflect those changes but any unchanged blocks continue to be shared no matter how many clones exist This is an implementation of the Copy on write principle br br br Sending and receiving snapshots br br ZFS file systems can be moved to other pools also on remote hosts over the network as the send command creates a stream representation of the file system s state This stream can either describe complete contents of the file system at a given snapshot or it can be a delta between snapshots Computing the delta stream is very efficient and its size depends on the number of blocks changed between the snapshots This provides an efficient strategy e g for synchronizing offsite backups or high availability mirrors of a pool br br br Dynamic striping br Dynamic striping across all devices to maximize throughput means that as additional devices are added to the zpool the stripe width automatically expands to include them thus all disks in a pool are used which balances the write load across them br br br Variable block sizes br ZFS uses variable sized blocks with KB as the default size Available features allow the administrator to tune the maximum block size which is used as certain workloads do not perform well with large blocks If data compression is enabled variable block sizes are used If a block can be compressed to fit into a smaller block size the smaller size is used on the disk to use less storage and improve IO throughput though at the cost of increased CPU use for the compression and decompression operations br br br Lightweight filesystem creation br In ZFS filesystem manipulation within a storage pool is easier than volume manipulation within a traditional filesystem the time and effort required to create or expand a ZFS filesystem is closer to that of making a new directory than it is to volume manipulation in some other systems br br br Adaptive endianness br br Pools and their associated ZFS file systems can be moved between different platform architectures including systems implementing different byte orders The ZFS block pointer format stores filesystem metadata in an endian adaptive way individual metadata blocks are written with the native byte order of the system writing the block When reading if the stored endianness does not match the endianness of the system the metadata is byte swapped in memory br This does not affect the stored data as is usual in POSIX systems files appear to applications as simple arrays of bytes so applications creating and reading data remain responsible for doing so in a way independent of the underlying system s endianness br br br Deduplication br Data deduplication capabilities were added to the ZFS source repository at the end of October and relevant OpenSolaris ZFS development packages have been available since December build br Effective use of deduplication may require large RAM capacity recommendations range between and GB of RAM for every TB of storage An accurate assessment of the memory required for deduplication is made by referring to the number of unique blocks in the pool and the number of bytes on disk and in RAM core required to store each record these figures are reported by inbuilt commands such as zpool and zdb Insufficient physical memory or lack of ZFS cache can result in virtual memory thrashing when using deduplication which can cause performance to plummet or result in complete memory starvation Because deduplication occurs at write time it is also very CPU intensive and this can also significantly slow down a system br Other storage vendors use modified versions of ZFS to achieve very high data compression ratios Two examples in were GreenBytes and Tegile In May Oracle bought GreenBytes for its ZFS deduplication and replication technology br As described above deduplication is usually not recommended due to its heavy resource requirements especially RAM and impact on performance especially when writing other than in specific circumstances where the system and data are well suited to this space saving technique br br br Additional capabilities br Explicit I O priority with deadline scheduling br Claimed globally optimal I O sorting and aggregation br Multiple independent prefetch streams with automatic length and stride detection br Parallel constant time directory operations br End to end checksumming using a kind of Data Integrity Field allowing data corruption detection and recovery if you have redundancy in the pool A choice of hashes can be used optimized for speed fletcher standardization and security SHA and salted hashes Skein br Transparent filesystem compression Supports LZJB gzip LZ and Zstd br Intelligent scrubbing and resilvering resyncing br Load and space usage sharing among disks in the pool br Ditto blocks Configurable data replication per filesystem with zero one or two extra copies requested per write for user data and with that same base number of copies plus one or two for metadata according to metadata importance If the pool has several devices ZFS tries to replicate over different devices Ditto blocks are primarily an additional protection against corrupted sectors not against total disk failure br ZFS design copy on write superblocks is safe when using disks with write cache enabled if they honor the write barriers This feature provides safety and a performance boost compared with some other filesystems br On Solaris when entire disks are added to a ZFS pool ZFS automatically enables their write cache This is not done when ZFS only manages discrete slices of the disk since it does not know if other slices are managed by non write cache safe filesystems like UFS The FreeBSD implementation can handle disk flushes for partitions thanks to its GEOM framework and therefore does not suffer from this limitation br Per user per group per project and per dataset quota limits br Filesystem encryption since Solaris Express and OpenZFS ZoL on some other systems ZFS can utilize encrypted disks for a similar effect GELI on FreeBSD can be used this way to create fully encrypted ZFS storage br Pools can be imported in read only mode br It is possible to recover data by rolling back entire transactions at the time of importing the zpool br ZFS is not a clustered filesystem however clustered ZFS is available from third parties br Snapshots can be taken manually or automatically The older versions of the stored data that they contain can be exposed as full read only file systems They can also be exposed as historic versions of files and folders when used with CIFS also known as SMB Samba or file shares this is known as Previous versions VSS shadow copies or File history on Windows or AFP and Apple Time Machine on Apple devices br Disks can be marked as spare A data pool can be set to automatically and transparently handle disk faults by activating a spare disk and beginning to resilver the data that was on the suspect disk onto it when needed br br br Limitations br As of Solaris Update and Solaris it was neither possible to reduce the number of top level vdevs in a pool except hot spares cache and log devices nor to otherwise reduce pool capacity This functionality was said to be in development in Enhancements to allow reduction of vdevs is under development in OpenZFS Online shrinking by removing non redundant top level vdevs is supported since Solaris released in August and OpenZFS ZoL released May br As of it was not possible to add a disk as a column to a RAID Z RAID Z or RAID Z vdev However a new RAID Z vdev can be created instead and added to the zpool br Some traditional nested RAID configurations such as RAID a mirror of RAID groups are not configurable in ZFS without some rd party tools Vdevs can only be composed of raw disks or files not other vdevs using the default ZFS management commands However a ZFS pool effectively creates a stripe RAID across its vdevs so the equivalent of a RAID or RAID is common br Reconfiguring the number of devices in a top level vdev requires copying data offline destroying the pool and recreating the pool with the new top level vdev configuration except for adding extra redundancy to an existing mirror which can be done at any time or if all top level vdevs are mirrors with sufficient redundancy the zpool split command can be used to remove a vdev from each top level vdev in the pool creating a nd pool with identical data br br br Data recovery br ZFS does not ship with tools such as fsck because the file system itself was designed to self repair So long as a storage pool had been built with sufficient attention to the design of storage and redundancy of data basic tools like fsck were never required However if the pool was compromised because of poor hardware inadequate design or redundancy or unfortunate mishap to the point that ZFS was unable to mount the pool traditionally there were no other more advanced tools which allowed an end user to attempt partial salvage of the stored data from a badly corrupted pool br Modern ZFS has improved considerably on this situation over time and continues to do so br br Removal or abrupt failure of caching devices no longer causes pool loss At worst loss of the ZIL may lose very recent transactions but the ZIL does not usually store more than a few seconds worth of recent transactions Loss of the L ARC cache does not affect data br If the pool is unmountable modern versions of ZFS will attempt to identify the most recent consistent point at which the pool can be recovered at the cost of losing some of the most recent changes to the contents Copy on write means that older versions of data including top level records and metadata may still exist even though they are superseded and if so the pool can be wound back to a consistent state based on them The older the data the more likely it is that at least some blocks have been overwritten and that some data will be irrecoverable so there is a limit at some point on the ability of the pool to be wound back br Informally tools exist to probe the reason why ZFS is unable to mount a pool and guide the user or a developer as to manual changes required to force the pool to mount These include using zdb ZFS debug to find a valid importable point in the pool using dtrace or similar to identify the issue causing mount failure or manually bypassing health checks that cause the mount process to abort and allow mounting of the damaged pool br As of March a range of significantly enhanced methods are gradually being rolled out within OpenZFS These include br Code refactoring and more detailed diagnostic and debug information on mount failures to simplify diagnosis and fixing of corrupt pool issues br The ability to trust or distrust the stored pool configuration This is particularly powerful as it allows a pool to be mounted even when top level vdevs are missing or faulty when top level data is suspect and also to rewind beyond a pool configuration change if that change was connected to the problem Once the corrupt pool is mounted readable files can be copied for safety and it may turn out that data can be rebuilt even for missing vdevs by using copies stored elsewhere in the pool br The ability to fix the situation where a disk needed in one pool was accidentally removed and added to a different pool causing it to lose metadata related to the first pool which becomes unreadable br br br OpenZFS and ZFS br Oracle Corporation ceased the public development of both ZFS and OpenSolaris after the acquisition of Sun in Some developers forked the last public release of OpenSolaris as the Illumos project Because of the significant advantages present in ZFS it has been ported to several different platforms with different features and commands For coordinating the development efforts and to avoid fragmentation OpenZFS was founded in br According to Matt Ahrens one of the main architects of ZFS over of the original OpenSolaris ZFS code has been replaced in OpenZFS with community contributions as of making Oracle ZFS and OpenZFS politically and technologically incompatible br br br Commercial and open source products br br Sun shipped a line of ZFS based series storage appliances br Oracle shipped ZS series of ZFS based filers and seized first place in the SPC benchmark with one of them br iXsystems ships ZFS based NAS devices called FreeNAS now named TrueNAS CORE for SOHO and TrueNAS for the enterprise br Netgear ships a line of ZFS based NAS devices called ReadyDATA designed to be used in the enterprise br rsync net announces a cloud storage platform that allows customers to provision their own zpool and import and export data using zfs send and zfs receive br iXsystems Begins development of a ZFS based hyperconverged software called TrueNAS SCALE for SOHO and TrueNAS for the enterprise br br br Oracle Corporation closed source and forking from br In January Oracle Corporation acquired Sun Microsystems and quickly discontinued the OpenSolaris distribution and the open source development model In August Oracle discontinued providing public updates to the source code of the Solaris OS Networking repository effectively turning Solaris back into a closed source proprietary operating system br In response to the changing landscape of Solaris and OpenSolaris the illumos project was launched via webinar on Thursday August as a community effort of some core Solaris engineers to continue developing the open source version of Solaris and complete the open sourcing of those parts not already open sourced by Sun illumos was founded as a Foundation the Illumos Foundation incorporated in the State of California as a c trade association The original plan explicitly stated that illumos would not be a distribution or a fork However after Oracle announced discontinuing OpenSolaris plans were made to fork the final version of the Solaris ON allowing illumos to evolve into an operating system of its own As part of OpenSolaris an open source version of ZFS was therefore integral within illumos br ZFS was widely used within numerous platforms as well as Solaris Therefore in the co ordination of development work on the open source version of ZFS was passed to an umbrella project OpenZFS The OpenZFS framework allows any interested parties to collaboratively develop the core ZFS codebase in common while individually maintaining any specific extra code which ZFS requires to function and integrate within their own systems br br br Version history br br Note The Solaris version under development by Sun since the release of Solaris in was codenamed Nevada and was derived from what was the OpenSolaris codebase Solaris Nevada is the codename for the next generation Solaris OS to eventually succeed Solaris and this new code was then pulled successively into new OpenSolaris Nevada snapshot builds OpenSolaris is now discontinued and OpenIndiana forked from it br A final build b of OpenSolaris was published by Oracle Nov as an upgrade path to Solaris Express br br br Operating system support br List of Operating Systems distributions and add ons that support ZFS the zpool version it supports and the Solaris build they are based on if any br br br See also br Comparison of file systems br List of file systems br Versioning file system List of versioning file systems br br br Notes br br br br br br Bibliography br br br External links br Fork Yeah The Rise and Development of Illumos slide show covering much of the history of Solaris the decision to open source by Sun the creation of ZFS and the events causing it to be closed sourced and forked after Oracle s acquisition br The best cloud File System was created before the cloud existed archived on Dec br Comparison of SVM mirroring and ZFS mirroring br EON ZFS Storage NAS distribution br End to end Data Integrity for File Systems A ZFS Case Study br ZFS The Zettabyte File System archived on Feb br ZFS and RAID Z The ber FS br ZFS The Last Word In File Systems by Jeff Bonwick and Bill Moore archived on Aug br Visualizing the ZFS intent log ZIL April by Aaron Toponce br Features of illumos including OpenZFS br Previous wiki page with more links Getting Started with ZFS Sep archived on Dec part of the illumos documentation