title: Cache (computing)
id: 6829
In computing a cache KASH is a hardware or software component that stores data so that future requests for that data can be served faster the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere A cache hit occurs when the requested data can be found in a cache while a cache miss occurs when it cannot Cache hits are served by reading data from the cache which is faster than recomputing a result or reading from a slower data store thus the more requests that can be served from the cache the faster the system performs br To be cost effective caches must be relatively small Nevertheless caches are effective in many areas of computing because typical computer applications access data with a high degree of locality of reference Such access patterns exhibit temporal locality where data is requested that has been recently requested and spatial locality where data is requested that is stored near data that has already been requested br br br Motivation br In memory design there is an inherent trade off between capacity and speed because larger capacity implies larger size and thus greater physical distances for signals to travel causing propagation delays There is also a tradeoff between high performance technologies such as SRAM and cheaper easily mass produced commodities such as DRAM flash or hard disks br The buffering provided by a cache benefits one or both of latency and throughput bandwidth br A larger resource incurs a significant latency for access e g it can take hundreds of clock cycles for a modern GHz processor to reach DRAM This is mitigated by reading large chunks into the cache in the hope that subsequent reads will be from nearby locations and can be read from the cache Prediction or explicit prefetching can be used to guess where future reads will come from and make requests ahead of time if done optimally the latency is bypassed altogether br The use of a cache also allows for higher throughput from the underlying resource by assembling multiple fine grain transfers into larger more efficient requests In the case of DRAM circuits the additional throughput may be gained by using a wider data bus br br br Operation br Hardware implements cache as a block of memory for temporary storage of data likely to be used again Central processing units CPUs solid state drives SSDs and hard disk drives HDDs frequently include hardware based cache while web browsers and web servers commonly rely on software caching br A cache is made up of a pool of entries Each entry has associated data which is a copy of the same data in some backing store Each entry also has a tag which specifies the identity of the data in the backing store of which the entry is a copy br When the cache client a CPU web browser operating system needs to access data presumed to exist in the backing store it first checks the cache If an entry can be found with a tag matching that of the desired data the data in the entry is used instead This situation is known as a cache hit For example a web browser program might check its local cache on disk to see if it has a local copy of the contents of a web page at a particular URL In this example the URL is the tag and the content of the web page is the data The percentage of accesses that result in cache hits is known as the hit rate or hit ratio of the cache br The alternative situation when the cache is checked and found not to contain any entry with the desired tag is known as a cache miss This requires a more expensive access of data from the backing store Once the requested data is retrieved it is typically copied into the cache ready for the next access br During a cache miss some other previously existing cache entry is typically removed in order to make room for the newly retrieved data The heuristic used to select the entry to replace is known as the replacement policy One popular replacement policy least recently used LRU replaces the oldest entry the entry that was accessed less recently than any other entry More sophisticated caching algorithms also take into account the frequency of use of entries br br br Writing policies br br When a system writes data to cache it must at some point write that data to the backing store as well The timing of this write is controlled by what is known as the write policy There are two basic writing approaches br br Write through write is done synchronously both to the cache and to the backing store br Write back initially writing is done only to the cache The write to the backing store is postponed until the modified content is about to be replaced by another cache block br A write back cache is more complex to implement since it needs to track which of its locations have been written over and mark them as dirty for later writing to the backing store The data in these locations are written back to the backing store only when they are evicted from the cache a process referred to as a lazy write For this reason a read miss in a write back cache will often require two memory backing store accesses to service one for the write back and one to retrieve the needed data Other policies may also trigger data write back The client may make many changes to data in the cache and then explicitly notify the cache to write back the data br Since no data is returned to the requester on write operations a decision needs to be made whether or not data would be loaded into the cache on write misses br br Write allocate also called fetch on write data at the missed write location is loaded to cache followed by a write hit operation In this approach write misses are similar to read misses br No write allocate also called write no allocate or write around data at the missed write location is not loaded to cache and is written directly to the backing store In this approach data is loaded into the cache on read misses only br Both write through and write back policies can use either of these write miss policies but usually they are paired br br A write back cache uses write allocate hoping for subsequent writes or even reads to the same location which is now cached br A write through cache uses no write allocate Here subsequent writes have no advantage since they still need to be written directly to the backing store br Entities other than the cache may change the data in the backing store in which case the copy in the cache may become out of date or stale Alternatively when the client updates the data in the cache copies of those data in other caches will become stale Communication protocols between the cache managers that keep the data consistent are associated with cache coherence br br br Prefetch br br On a cache read miss caches with a demand paging policy read the minimum amount from the backing store A typical demand paging virtual memory implementation reads one page of virtual memory often KB from disk into the disk cache in RAM A typical CPU reads a single L cache line of bytes from DRAM into the L cache and a single L cache line of bytes from the L cache into the L cache br Caches with a prefetch input queue or more general anticipatory paging policy go further they not only read the data requested but guess that the next chunk or two of data will soon be required and so prefetch that data into the cache ahead of time Anticipatory paging is especially helpful when the backing store has a long latency to read the first chunk and much shorter times to sequentially read the next few chunks such as disk storage and DRAM br A few operating systems go further with a loader that always pre loads the entire executable into RAM A few caches go even further not only pre loading an entire file but also starting to load other related files that may soon be requested such as the page cache associated with a prefetcher or the web cache associated with link prefetching br br br Examples of hardware caches br br br CPU cache br br Small memories on or close to the CPU can operate faster than the much larger main memory Most CPUs since the s have used one or more caches sometimes in cascaded levels modern high end embedded desktop and server microprocessors may have as many as six types of cache between levels and functions Some examples of caches with a specific function are the D cache I cache and the translation lookaside buffer for the memory management unit MMU br br br GPU cache br Earlier graphics processing units GPUs often had limited read only texture caches and used Swizzling computer graphics to improve D locality of reference Cache misses would drastically affect performance e g if mipmapping was not used Caching was important to leverage bit and wider transfers for texture data that was often as little as bits per pixel br As GPUs advanced supporting general purpose computing on graphics processing units and compute kernels they have developed progressively larger and increasingly general caches including instruction caches for shaders exhibiting functionality commonly found in CPU caches These caches have grown to handle synchronization primitives between threads and atomic operations and interface with a CPU style MMU br br br DSPs br Digital signal processors have similarly generalized over the years Earlier designs used scratchpad memory fed by direct memory access but modern DSPs such as Qualcomm Hexagon often include a very similar set of caches to a CPU e g Modified Harvard architecture with shared L split L I cache and D cache br br br Translation lookaside buffer br br A memory management unit MMU that fetches page table entries from main memory has a specialized cache used for recording the results of virtual address to physical address translations This specialized cache is called a translation lookaside buffer TLB br br br In network cache br br br Information centric networking br Information centric networking ICN is an approach to evolve the Internet infrastructure away from a host centric paradigm based on perpetual connectivity and the end to end principle to a network architecture in which the focal point is identified information or content or data Due to the inherent caching capability of the nodes in an ICN it can be viewed as a loosely connected network of caches which has unique requirements of caching policies However ubiquitous content caching introduces the challenge to content protection against unauthorized access which requires extra care and solutions br Unlike proxy servers in ICN the cache is a network level solution Therefore it has rapidly changing cache states and higher request arrival rates moreover smaller cache sizes further impose a different kind of requirements on the content eviction policies In particular eviction policies for ICN should be fast and lightweight Various cache replication and eviction schemes for different ICN architectures and applications have been proposed br br br Policies br br br Time aware least recently used TLRU br The Time aware Least Recently Used TLRU is a variant of LRU designed for the situation where the stored contents in cache have a valid life time The algorithm is suitable in network cache applications such as ICN content delivery networks CDNs and distributed networks in general TLRU introduces a new term TTU Time to Use TTU is a time stamp of a content page which stipulates the usability time for the content based on the locality of the content and the content publisher announcement Owing to this locality based time stamp TTU provides more control to the local administrator to regulate in network storage br In the TLRU algorithm when a piece of content arrives a cache node calculates the local TTU value based on the TTU value assigned by the content publisher The local TTU value is calculated by using a locally defined function Once the local TTU value is calculated the replacement of content is performed on a subset of the total content stored in cache node The TLRU ensures that less popular and small life content should be replaced with the incoming content br br br Least frequent recently used LFRU br The Least Frequent Recently Used LFRU cache replacement scheme combines the benefits of LFU and LRU schemes LFRU is suitable for in network cache applications such as ICN CDNs and distributed networks in general In LFRU the cache is divided into two partitions called privileged and unprivileged partitions The privileged partition can be defined as a protected partition If content is highly popular it is pushed into the privileged partition Replacement of the privileged partition is done as follows LFRU evicts content from the unprivileged partition pushes content from privileged partition to unprivileged partition and finally inserts new content into the privileged partition In the above procedure the LRU is used for the privileged partition and an approximated LFU ALFU scheme is used for the unprivileged partition hence the abbreviation LFRU The basic idea is to filter out the locally popular contents with ALFU scheme and push the popular contents to one of the privileged partition br br br Weather forecast br In the use of smartphones with weather forecasting options was overly taxing AccuWeather servers two requests within the same park would generate separate requests An optimization by edge servers to truncate the GPS coordinates to fewer decimal places meant that the cached results from the earlier query would be used The number of to the server lookups per day dropped by half br br br Software caches br br br Disk cache br br While CPU caches are generally managed entirely by hardware a variety of software manages other caches The page cache in main memory which is an example of disk cache is managed by the operating system kernel br While the disk buffer which is an integrated part of the hard disk drive or solid state drive is sometimes misleadingly referred to as disk cache its main functions are write sequencing and read prefetching Repeated cache hits are relatively rare due to the small size of the buffer in comparison to the drive s capacity However high end disk controllers often have their own on board cache of the hard disk drive s data blocks br Finally a fast local hard disk drive can also cache information held on even slower data storage devices such as remote servers web cache or local tape drives or optical jukeboxes such a scheme is the main concept of hierarchical storage management Also fast flash based solid state drives SSDs can be used as caches for slower rotational media hard disk drives working together as hybrid drives or solid state hybrid drives SSHDs br br br Web cache br br Web browsers and web proxy servers employ web caches to store previous responses from web servers such as web pages and images Web caches reduce the amount of information that needs to be transmitted across the network as information previously stored in the cache can often be re used This reduces bandwidth and processing requirements of the web server and helps to improve responsiveness for users of the web br Web browsers employ a built in web cache but some Internet service providers ISPs or organizations also use a caching proxy server which is a web cache that is shared among all users of that network br Another form of cache is P P caching where the files most sought for by peer to peer applications are stored in an ISP cache to accelerate P P transfers Similarly decentralised equivalents exist which allow communities to perform the same task for P P traffic for example Corelli br br br Memoization br br A cache can store data that is computed on demand rather than retrieved from a backing store Memoization is an optimization technique that stores the results of resource consuming function calls within a lookup table allowing subsequent calls to reuse the stored results and avoid repeated computation It is related to the dynamic programming algorithm design methodology which can also be thought of as a means of caching br br br Content delivery network br br A content delivery network CDN is a network of distributed servers that deliver pages and other Web content to a user based on the geographic locations of the user the origin of the web page and the content delivery server br CDNs began in the late s as a way to speed up the delivery of static content such as HTML pages images and videos By replicating content on multiple servers around the world and delivering it to users based on their location CDNs can significantly improve the speed and availability of a website or application When a user requests a piece of content the CDN will check to see if it has a copy of the content in its cache If it does the CDN will deliver the content to the user from the cache br br br Cloud storage gateway br br A cloud storage gateway also known as an edge filer is a hybrid cloud storage device that connects a local network to one or more cloud storage services typically object storage services such as Amazon S It provides a cache for frequently accessed data providing high speed local access to frequently accessed data in the cloud storage service Cloud storage gateways also provide additional benefits such as accessing cloud object storage through traditional file serving protocols as well as continued access to cached data during connectivity outages br br br Other caches br The BIND DNS daemon caches a mapping of domain names to IP addresses as does a resolver library br Write through operation is common when operating over unreliable networks like an Ethernet LAN because of the enormous complexity of the coherency protocol required between multiple write back caches when communication is unreliable For instance web page caches and client side network file system caches like those in NFS or SMB are typically read only or write through specifically to keep the network protocol simple and reliable br Search engines also frequently make web pages they have indexed available from their cache For example Google provides a Cached link next to each search result This can prove useful when web pages from a web server are temporarily or permanently inaccessible br Database caching can substantially improve the throughput of database applications for example in the processing of indexes data dictionaries and frequently used subsets of data br A distributed cache uses networked hosts to provide scalability reliability and performance to the application The hosts can be co located or spread over different geographical regions br br br Buffer vs cache br br The semantics of a buffer and a cache are not totally different even so there are fundamental differences in intent between the process of caching and the process of buffering br Fundamentally caching realizes a performance increase for transfers of data that is being repeatedly transferred While a caching system may realize a performance increase upon the initial typically write transfer of a data item this performance increase is due to buffering occurring within the caching system br With read caches a data item must have been fetched from its residing location at least once in order for subsequent reads of the data item to realize a performance increase by virtue of being able to be fetched from the cache s faster intermediate storage rather than the data s residing location With write caches a performance increase of writing a data item may be realized upon the first write of the data item by virtue of the data item immediately being stored in the cache s intermediate storage deferring the transfer of the data item to its residing storage at a later stage or else occurring as a background process Contrary to strict buffering a caching process must adhere to a potentially distributed cache coherency protocol in order to maintain consistency between the cache s intermediate storage and the location where the data resides Buffering on the other hand br br reduces the number of transfers for otherwise novel data amongst communicating processes which amortizes overhead involved for several small transfers over fewer larger transfers br provides an intermediary for communicating processes which are incapable of direct transfers amongst each other or br ensures a minimum data size or representation required by at least one of the communicating processes involved in a transfer br With typical caching implementations a data item that is read or written for the first time is effectively being buffered and in the case of a write mostly realizing a performance increase for the application from where the write originated Additionally the portion of a caching protocol where individual writes are deferred to a batch of writes is a form of buffering The portion of a caching protocol where individual reads are deferred to a batch of reads is also a form of buffering although this form may negatively impact the performance of at least the initial reads even though it may positively impact the performance of the sum of the individual reads In practice caching almost always involves some form of buffering while strict buffering does not involve caching br A buffer is a temporary memory location that is traditionally used because CPU instructions cannot directly address data stored in peripheral devices Thus addressable memory is used as an intermediate stage Additionally such a buffer may be feasible when a large block of data is assembled or disassembled as required by a storage device or when data may be delivered in a different order than that in which it is produced Also a whole buffer of data is usually transferred sequentially for example to hard disk so buffering itself sometimes increases transfer performance or reduces the variation or jitter of the transfer s latency as opposed to caching where the intent is to reduce the latency These benefits are present even if the buffered data are written to the buffer once and read from the buffer once br A cache also increases transfer performance A part of the increase similarly comes from the possibility that multiple small transfers will combine into one large block But the main performance gain occurs because there is a good chance that the same data will be read from cache multiple times or that written data will soon be read A cache s sole purpose is to reduce accesses to the underlying slower storage Cache is also usually an abstraction layer that is designed to be invisible from the perspective of neighboring layers br br br See also br br br br br br Further reading br What Every Programmer Should Know About Memory br Caching in the Distributed Environment 