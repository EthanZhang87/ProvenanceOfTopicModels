title: Texture mapping
id: 146903
Texture mapping is a method for mapping a texture on a computer generated graphic Texture in this context can be high frequency detail surface texture or color br br br History br The original technique was pioneered by Edwin Catmull in as part of his doctoral thesis br Texture mapping originally referred to diffuse mapping a method that simply mapped pixels from a texture to a D surface wrapping the image around the object In recent decades the advent of multi pass rendering multitexturing mipmaps and more complex mappings such as height mapping bump mapping normal mapping displacement mapping reflection mapping specular mapping occlusion mapping and many other variations on the technique controlled by a materials system have made it possible to simulate near photorealism in real time by vastly reducing the number of polygons and lighting calculations needed to construct a realistic and functional D scene br br br Texture maps br br A texture map is an image applied mapped to the surface of a shape or polygon This may be a bitmap image or a procedural texture They may be stored in common image file formats referenced by D model formats or material definitions and assembled into resource bundles br They may have one to three dimensions although two dimensions are most common for visible surfaces For use with modern hardware texture map data may be stored in swizzled or tiled orderings to improve cache coherency Rendering APIs typically manage texture map resources which may be located in device memory as buffers or surfaces and may allow render to texture for additional effects such as post processing or environment mapping br They usually contain RGB color data either stored as direct color compressed formats or indexed color and sometimes an additional channel for alpha blending RGBA especially for billboards and decal overlay textures It is possible to use the alpha channel which may be convenient to store in formats parsed by hardware for other uses such as specularity br Multiple texture maps or channels may be combined for control over specularity normals displacement or subsurface scattering e g for skin rendering br Multiple texture images may be combined in texture atlases or array textures to reduce state changes for modern hardware They may be considered a modern evolution of tile map graphics Modern hardware often supports cube map textures with multiple faces for environment mapping br br br Creation br Texture maps may be acquired by scanning digital photography designed in image manipulation software such as GIMP Photoshop or painted onto D surfaces directly in a D paint tool such as Mudbox or zbrush br br br Texture application br This process is akin to applying patterned paper to a plain white box Every vertex in a polygon is assigned a texture coordinate which in the d case is also known as UV coordinates br This may be done through explicit assignment of vertex attributes manually edited in a D modelling package through UV unwrapping tools It is also possible to associate a procedural transformation from D space to texture space with the material This might be accomplished via planar projection or alternatively cylindrical or spherical mapping More complex mappings may consider the distance along a surface to minimize distortion br These coordinates are interpolated across the faces of polygons to sample the texture map during rendering br Textures may be repeated or mirrored to extend a finite rectangular bitmap over a larger area or they may have a one to one unique injective mapping from every piece of a surface which is important for render mapping and light mapping also known as baking br br br Texture space br Texture mapping maps the model surface or screen space during rasterization into texture space in this space the texture map is visible in its undistorted form UV unwrapping tools typically provide a view in texture space for manual editing of texture coordinates Some rendering techniques such as subsurface scattering may be performed approximately by texture space operations br br br Multitexturing br Multitexturing is the use of more than one texture at a time on a polygon For instance a light map texture may be used to light a surface as an alternative to recalculating that lighting every time the surface is rendered Microtextures or detail textures are used to add higher frequency details and dirt maps may add weathering and variation this can greatly reduce the apparent periodicity of repeating textures Modern graphics may use more than layers which are combined using shaders for greater fidelity Another multitexture technique is bump mapping which allows a texture to directly control the facing direction of a surface for the purposes of its lighting calculations it can give a very good appearance of a complex surface such as tree bark or rough concrete that takes on lighting detail in addition to the usual detailed coloring Bump mapping has become popular in recent video games as graphics hardware has become powerful enough to accommodate it in real time br br br Texture filtering br The way that samples e g when viewed as pixels on the screen are calculated from the texels texture pixels is governed by texture filtering The cheapest method is to use the nearest neighbour interpolation but bilinear interpolation or trilinear interpolation between mipmaps are two commonly used alternatives which reduce aliasing or jaggies In the event of a texture coordinate being outside the texture it is either clamped or wrapped Anisotropic filtering better eliminates directional artefacts when viewing textures from oblique viewing angles br br br Texture streaming br Texture streaming is a means of using data streams for textures where each texture is available in two or more different resolutions as to determine which texture should be loaded into memory and used based on draw distance from the viewer and how much memory is available for textures Texture streaming allows a rendering engine to use low resolution textures for objects far away from the viewer s camera and resolve those into more detailed textures read from a data source as the point of view nears the objects br br br Baking br As an optimization it is possible to render detail from a complex high resolution model or expensive process such as global illumination into a surface texture possibly on a low resolution model Baking is also known as render mapping This technique is most commonly used for light maps but may also be used to generate normal maps and displacement maps Some computer games e g Messiah have used this technique The original Quake software engine used on the fly baking to combine light maps and colour maps surface caching br Baking can be used as a form of level of detail generation where a complex scene with many different elements and materials may be approximated by a single element with a single texture which is then algorithmically reduced for lower rendering cost and fewer drawcalls It is also used to take high detail models from D sculpting software and point cloud scanning and approximate them with meshes more suitable for realtime rendering br br br Rasterisation algorithms br Various techniques have evolved in software and hardware implementations Each offers different trade offs in precision versatility and performance br br br Affine texture mapping br br Affine texture mapping linearly interpolates texture coordinates across a surface and so is the fastest form of texture mapping Some software and hardware such as the original PlayStation project vertices in D space onto the screen during rendering and linearly interpolate the texture coordinates in screen space between them This may be done by incrementing fixed point UV coordinates or by an incremental error algorithm akin to Bresenham s line algorithm br In contrast to perpendicular polygons this leads to noticeable distortion with perspective transformations see figure the checker box texture appears bent especially as primitives near the camera Such distortion may be reduced with the subdivision of the polygon into smaller ones br For the case of rectangular objects using quad primitives can look less incorrect than the same rectangle split into triangles but because interpolating points adds complexity to the rasterization most early implementations preferred triangles only Some hardware such as the forward texture mapping used by the Nvidia NV was able to offer efficient quad primitives With perspective correction see below triangles become equivalent and this advantage disappears br br For rectangular objects that are at right angles to the viewer like floors and walls the perspective only needs to be corrected in one direction across the screen rather than both The correct perspective mapping can be calculated at the left and right edges of the floor and then an affine linear interpolation across that horizontal span will look correct because every pixel along that line is the same distance from the viewer br br br Perspective correctness br Perspective correct texturing accounts for the vertices positions in D space rather than simply interpolating coordinates in D screen space This achieves the correct visual effect but it is more expensive to calculate br To perform perspective correction of the texture coordinates br br br br u br br br displaystyle u br br and br br br br v br br br displaystyle v br br with br br br br z br br br displaystyle z br br being the depth component from the viewer s point of view we can take advantage of the fact that the values br br br br br br br z br br br br br displaystyle frac z br br br br br br br br u br z br br br br br displaystyle frac u z br br and br br br br br br v br z br br br br br displaystyle frac v z br br are linear in screen space across the surface being textured In contrast the original br br br br z br br br displaystyle z br br br br br br u br br br displaystyle u br br and br br br br v br br br displaystyle v br br before the division are not linear across the surface in screen space We can therefore linearly interpolate these reciprocals across the surface computing corrected values at each pixel to result in a perspective correct texture mapping br To do this we first calculate the reciprocals at each vertex of our geometry points for a triangle For vertex br br br br n br br br displaystyle n br br we have br br br br br br br u br br n br br br br z br br n br br br br br br br br br v br br n br br br br z br br n br br br br br br br br br br z br br n br br br br br br br displaystyle frac u n z n frac v n z n frac z n br br Then we linearly interpolate these reciprocals between the br br br br n br br br displaystyle n br br vertices e g using barycentric coordinates resulting in interpolated values across the surface At a given point this yields the interpolated br br br br br u br br i br br br br br v br br i br br br br br displaystyle u i v i br br and br br br br z br R br e br c br i br p br r br o br c br a br br l br br i br br br br br br br br z br br i br br br br br br br displaystyle zReciprocal i frac z i br br Note that this br br br br br u br br i br br br br br v br br i br br br br br displaystyle u i v i br br cannot be yet used as our texture coordinates as our division by br br br br z br br br displaystyle z br br altered their coordinate system br To correct back to the br br br br u br br v br br br displaystyle u v br br space we first calculate the corrected br br br br z br br br displaystyle z br br by again taking the reciprocal br br br br br z br br c br o br r br r br e br c br t br br br br br br br br z br R br e br c br i br p br r br o br c br a br br l br br i br br br br br br br br br br br br br z br br i br br br br br br br br displaystyle z correct frac zReciprocal i frac frac z i br br Then we use this to correct our br br br br br u br br i br br br br br v br br i br br br br br displaystyle u i v i br br br br br br br u br br c br o br r br r br e br c br t br br br br br u br br i br br br br br z br br i br br br br br displaystyle u correct u i cdot z i br br and br br br br br v br br c br o br r br r br e br c br t br br br br br v br br i br br br br br z br br i br br br br br displaystyle v correct v i cdot z i br br br This correction makes it so that in parts of the polygon that are closer to the viewer the difference from pixel to pixel between texture coordinates is smaller stretching the texture wider and in parts that are farther away this difference is larger compressing the texture br br Affine texture mapping directly interpolates a texture coordinate br br br br br u br br br br br br br br br br displaystyle u alpha br br between two endpoints br br br br br u br br br br br br br br br br displaystyle u br br and br br br br br u br br br br br br br br br br displaystyle u br br br br br br br br u br br br br br br br br br br br br br br br u br br br br br br br br u br br br br br br br displaystyle u alpha alpha u alpha u br br where br br br br br br br br br br br displaystyle leq alpha leq br br br Perspective correct mapping interpolates after dividing by depth br br br br br z br br br br br br br br br br displaystyle z br br then uses its interpolated reciprocal to recover the correct coordinate br br br br br br u br br br br br br br br br br br br br br br br br br br br u br br br br br br z br br br br br br br br br br br br u br br br br br br z br br br br br br br br br br br br br br br br br br z br br br br br br br br br br br br br z br br br br br br br br br br br br displaystyle u alpha frac alpha frac u z alpha frac u z alpha frac z alpha frac z br br br D graphics hardware typically supports perspective correct texturing br Various techniques have evolved for rendering texture mapped geometry into images with different quality precision tradeoffs which can be applied to both software and hardware br Classic software texture mappers generally did only simple mapping with at most one lighting effect typically applied through a lookup table and the perspective correctness was about times more expensive br br br Restricted camera rotation br br The Doom engine restricted the world to vertical walls and horizontal floors ceilings with a camera that could only rotate about the vertical axis This meant the walls would be a constant depth coordinate along a vertical line and the floors ceilings would have a constant depth along a horizontal line After performing one perspective correction calculation for the depth the rest of the line could use fast affine mapping Some later renderers of this era simulated a small amount of camera pitch with shearing which allowed the appearance of greater freedom whilst using the same rendering technique br Some engines were able to render texture mapped Heightmaps e g Nova Logic s Voxel Space and the engine for Outcast via Bresenham like incremental algorithms producing the appearance of a texture mapped landscape without the use of traditional geometric primitives br br br Subdivision for perspective correction br Every triangle can be further subdivided into groups of about pixels in order to achieve two goals First keeping the arithmetic mill busy at all times Second producing faster arithmetic results br br br World space subdivision br For perspective texture mapping without hardware support a triangle is broken down into smaller triangles for rendering and affine mapping is used on them The reason this technique works is that the distortion of affine mapping becomes much less noticeable on smaller polygons The Sony PlayStation made extensive use of this because it only supported affine mapping in hardware but had a relatively high triangle throughput compared to its peers br br br Screen space subdivision br br Software renderers generally preferred screen subdivision because it has less overhead Additionally they try to do linear interpolation along a line of pixels to simplify the set up compared to d affine interpolation and thus again the overhead also affine texture mapping does not fit into the low number of registers of the x CPU the or any RISC is much more suited br A different approach was taken for Quake which would calculate perspective correct coordinates only once every pixels of a scanline and linearly interpolate between them effectively running at the speed of linear interpolation because the perspective correct calculation runs in parallel on the co processor The polygons are rendered independently hence it may be possible to switch between spans and columns or diagonal directions depending on the orientation of the polygon normal to achieve a more constant z but the effort seems not to be worth it br br br Other techniques br Another technique was approximating the perspective with a faster calculation such as a polynomial Still another technique uses z value of the last two drawn pixels to linearly extrapolate the next value The division is then done starting from those values so that only a small remainder has to be divided but the amount of bookkeeping makes this method too slow on most systems br Finally the Build engine extended the constant distance trick used for Doom by finding the line of constant distance for arbitrary polygons and rendering along it br br br Hardware implementations br Texture mapping hardware was originally developed for simulation e g as implemented in the Evans and Sutherland ESIG and Singer Link Digital Image Generators DIG and professional graphics workstations such as Silicon Graphics broadcast digital video effects machines such as the Ampex ADO and later appeared in Arcade cabinets consumer video game consoles and PC video cards in the mid s In flight simulation texture mapping provided important motion and altitude cues necessary for pilot training not available on untextured surfaces It was also in flight simulation applications that texture mapping was implemented for real time processing with prefiltered texture patterns stored in memory for real time access by the video processor br Modern graphics processing units GPUs provide specialised fixed function units called texture samplers or texture mapping units to perform texture mapping usually with trilinear filtering or better multi tap anisotropic filtering and hardware for decoding specific formats such as DXTn As of texture mapping hardware is ubiquitous as most SOCs contain a suitable GPU br Some hardware combines texture mapping with hidden surface determination in tile based deferred rendering or scanline rendering such systems only fetch the visible texels at the expense of using greater workspace for transformed vertices Most systems have settled on the Z buffering approach which can still reduce the texture mapping workload with front to back sorting br Among earlier graphics hardware there were two competing paradigms of how to deliver a texture to the screen br br Forward texture mapping iterates through each texel on the texture and decides where to place it on the screen br Inverse texture mapping instead iterates through pixels on the screen and decides what texel to use for each br Inverse texture mapping is the method which has become standard in modern hardware br br br Inverse texture mapping br With this method a pixel on the screen is mapped to a point on the texture Each vertex of a rendering primitive is projected to a point on the screen and each of these points is mapped to a u v texel coordinate on the texture A rasterizer will interpolate between these points to fill in each pixel covered by the primitive br The primary advantage is that each pixel covered by a primitive will be traversed exactly once Once a primitive s vertices are transformed the amount of remaining work scales directly with how many pixels it covers on the screen br The main disadvantage versus forward texture mapping is that the memory access pattern in the texture space will not be linear if the texture is at an angle to the screen This disadvantage is often addressed by texture caching techniques such as the swizzled texture memory arrangement br The linear interpolation can be used directly for simple and efficient affine texture mapping but can also be adapted for perspective correctness br br br Forward texture mapping br Forward texture mapping maps each texel of the texture to a pixel on the screen After transforming a rectangular primitive to a place on the screen a forward texture mapping renderer iterates through each texel on the texture splatting each one onto a pixel of the frame buffer br This was used by some hardware such as the DO the Sega Saturn and the NV br The primary advantage is that the texture will be accessed in a simple linear order allowing very efficient caching of the texture data However this benefit is also its disadvantage as a primitive gets smaller on screen it still has to iterate over every texel in the texture causing many pixels to be overdrawn redundantly br This method is also well suited for rendering quad primitives rather than reducing them to triangles which provided an advantage when perspective correct texturing was not available in hardware This is because the affine distortion of a quad looks less incorrect than the same quad split into two triangles see affine texture mapping above The NV hardware also allowed a quadratic interpolation mode to provide an even better approximation of perspective correctness br The existing hardware implementations did not provide effective UV coordinate mapping which became an important technique for D modelling and assisted in clipping the texture correctly when the primitive goes over the edge of the screen These shortcomings could have been addressed with further development but GPU design has since mostly moved toward inverse mapping br br br Applications br Beyond D rendering the availability of texture mapping hardware has inspired its use for accelerating other tasks br br br Tomography br It is possible to use texture mapping hardware to accelerate both the reconstruction of voxel data sets from tomographic scans and to visualize the results br br br User interfaces br Many user interfaces use texture mapping to accelerate animated transitions of screen elements e g Expos in Mac OS X br br br See also br D br D computer graphics br Mipmap br Materials system br Parametrization br Texture synthesis br Texture atlas br Texture splatting a technique for combining textures br Shader computer graphics br br br br br br Software br TexRecon Archived at the Wayback Machine open source software for texturing D models written in C br br br External links br Introduction into texture mapping using C and SDL PDF br Programming a textured terrain using XNA DirectX from www riemers net br Perspective correct texturing br Time Texturing Texture mapping with bezier lines br Polynomial Texture Mapping Archived at the Wayback Machine Interactive Relighting for Photos br M todos de interpolaci n a partir de puntos in spanish Methods that can be used to interpolate a texture knowing the texture coords at the vertices of a polygon br D Texturing Tools