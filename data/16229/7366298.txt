title: Solid-state drive
id: 7366298
A solid state drive SSD is a solid state storage device It provides persistent data storage using no moving parts It is sometimes called semiconductor storage device or solid state device It is also called solid state disk because it is frequently interfaced to a host system in the same manner as a hard disk drive HDD br An SSD is often used as secondary storage to provide relatively fast persistent direct attached storage in a computer br br br Attributes br An SSD has rich internal parallelism for data processing br In comparison to HDDs and similar electromechanical magnetic storage which have moving parts SSDs are typically silent resistant to physical shock and in large part due to lower latency have higher input output rates br An SSD stores data in semiconductor cells and an SSD s properties vary according to the number of bits stored in each cell which varies between and Single bit cells Single Level Cells or SLC are generally the most reliable durable fast and expensive type compared with and bit cells Multi Level Cells MLC and Triple Level Cells TLC and finally quad bit cells QLC being used for consumer devices that do not require such extreme properties and are the cheapest per storage unit In addition D XPoint memory sold by Intel under the Optane brand stores data by changing the electrical resistance of cells instead of storing electrical charges in cells and SSDs made from RAM can be used for high speed when data persistence after power loss is not required or may use battery power to retain data when its usual power source is unavailable Hybrid drives or solid state hybrid drives SSHDs such as Intel s Hystor and Apple s Fusion Drive combine features of SSDs and HDDs in the same unit using both flash memory and spinning magnetic disks in order to improve the performance of frequently accessed data Bcache achieves a similar effect purely in software using combinations of dedicated regular SSDs and HDDs br SSDs based on NAND flash slowly leak charge when not powered Heavily used consumer drives that have exceeded their endurance rating may start losing data typically after one year if stored at C to two years at C in storage for new drives it may take longer Enterprise drives may see data loss as early as months of no power stored at C Therefore SSDs are not suitable for archival storage br SSDs can use traditional HDD interfaces and form factors or newer interfaces and form factors that exploit specific advantages of the flash memory in SSDs Traditional interfaces e g SATA and SAS and standard HDD form factors allow such SSDs to be used as drop in replacements for HDDs in computers and other devices Newer form factors such as mSATA M U NF M NGSFF XFM Express Crossover Flash Memory form factor XT and EDSFF formerly known as Ruler SSD and higher speed interfaces such as NVM Express NVMe over PCI Express PCIe can further increase performance over HDD performance SSDs have a limited lifetime number of writes and also slow down as they reach their full storage capacity br br br Development and history br br br Early SSDs using RAM and similar technology br An early if not the first semiconductor storage device compatible with a hard drive interface e g an SSD as defined was the StorageTek STC a plug compatible replacement for the IBM fixed hard disk drive It initially used charge coupled devices CCDs for storage later switched to DRAM and consequently was reported to be seven times faster than the IBM product at about half the price for MB capacity Before the StorageTek SSD there were many DRAM and core e g DATARAM BULK Core products sold as alternatives to HDDs but they typically had memory interfaces and were not SSDs as defined br In the late s Zitel offered a family of DRAM based SSD products under the trade name RAMDisk for use on systems by UNIVAC and Perkin Elmer among others br br br SSDs using Flash br br The basis for flash based SSDs flash memory was invented by Fujio Masuoka at Toshiba in and commercialized by Toshiba in SanDisk Corporation then SunDisk founders Eli Harari and Sanjay Mehrotra along with Robert D Norman saw the potential of flash memory as an alternative to existing hard drives and filed a patent for a flash based SSD in The first commercial flash based SSD was shipped by SanDisk in It was a MB SSD in a PCMCIA configuration and sold OEM for around and was used by IBM in a ThinkPad laptop In SanDisk introduced SSDs in inch and inch form factors with PATA interfaces br In STEC Inc entered the flash memory business for consumer electronic devices br In M Systems introduced flash based solid state drives as HDD replacements for the military and aerospace industries as well as for other mission critical applications These applications require the SSD s ability to withstand extreme shock vibration and temperature ranges br In BiTMICRO made a number of introductions and announcements about flash based SSDs including an GB inch SSD In Fusion io announced a PCIe based Solid state drive with input output operations per second IOPS of performance in a single card with capacities up to GB br At Cebit OCZ Technology demonstrated a TB flash SSD using a PCI Express interface It achieved a maximum write speed of gigabytes per second GB s and maximum read speed of GB s In December Micron Technology announced an SSD using a gigabits per second Gbit s SATA interface br In Seagate demonstrated GB s sequential read and write speeds from a lane PCIe SSD and a TB SSD in a inch form factor Samsung also launched to market a TB SSD with a price tag of US using a SAS interface using a inch form factor but with the thickness of inch drives This was the first time a commercially available SSD had more capacity than the largest currently available HDD br In both Samsung and Toshiba launched TB SSDs using the same inch form factor but with inch drive thickness using a SAS interface Nimbus Data announced and reportedly shipped TB drives using a SATA interface a capacity HDDs are not expected to reach until Samsung introduced an M NVMe SSD with read speeds of GB s and write speeds of GB s A new version of the TB SSD was launched in at a price of US with the TB version costing US br In Gigabyte Technology demonstrated an TB lane PCIe SSD with GB s sequential read and GB s sequential write speeds at Computex It included a fan as new high speed SSDs run at high temperatures Also in NVMe M SSDs using the PCIe interface were launched These SSDs have read speeds of up to GB s and write speeds of up to GB s Due to their high speed operation these SSDs use large heatsinks and without sufficient cooling airflow will typically thermally throttle down after roughly minutes of continuous operation at full speed Samsung also introduced SSDs capable of GB s sequential read and write speeds and million IOPS capable of moving data from damaged chips to undamaged chips to allow the SSD to continue working normally albeit at a lower capacity br in NVMe was announced with Zoned Namespaces ZNS which br allows data to be mapped directly to its physical location in flash memory to directly access data on an SSD without a flash translation layer br In Samsung announced what it called the world s first SSD with a hybrid PCIe interface the Samsung EVO The hybrid interface runs in either the x PCIe or x PCIe modes a first for an M SSD br br br Enterprise flash drives br br Enterprise flash drives EFDs are designed for applications requiring high I O performance IOPS reliability energy efficiency and more recently consistent performance In most cases an EFD is an SSD with a higher set of specifications compared with SSDs that would typically be used in notebook computers The term was first used by EMC in January to identify SSD manufacturers who would provide products meeting these higher standards There are no standards bodies who control the definition of EFDs so any SSD manufacturer may claim to produce EFDs when in fact the product may not meet any particular requirements br An example is the Intel DC S series of drives introduced in the fourth quarter of which focuses on achieving consistent performance an area that had not received much attention but which Intel claimed was important for the enterprise market In particular Intel claims that at a steady state the S drives would not vary their IOPS by more than and that of all KB random I Os are serviced in less than s br Another example is the Toshiba PX SS enterprise SSD series announced in optimized for use in server and storage platforms requiring high endurance from write intensive applications such as write caching I O acceleration and online transaction processing OLTP The PX SS series uses Gbit s SAS interface featuring MLC NAND flash memory and achieving random write speeds of up to IOPS random read speeds of up to IOPS and endurance rating of drive writes per day DWPD br br br Drives using other persistent memory technologies br In the first products with D XPoint memory were released under Intel s Optane brand D Xpoint is entirely different from NAND flash and stores data using different principles SSDs based on D XPoint have higher IOPS up to million but lower sequential read write speeds than their NAND flash counterparts br br br Architecture and function br The key components of an SSD are the controller and the memory to store the data The primary memory component in an SSD was traditionally DRAM volatile memory but since it is more commonly NAND flash non volatile memory br br br Controller br Every SSD includes a controller that incorporates the electronics that bridge the NAND memory components to the host computer The controller is an embedded processor that executes firmware level code and is one of the most important factors of SSD performance Some of the functions performed by the controller include br br Bad block mapping br Read and write caching br Encryption br Crypto shredding br Error detection and correction via error correcting code ECC such as BCH code br Garbage collection br Read scrubbing and read disturb management br Wear leveling br The performance of an SSD can scale with the number of parallel NAND flash chips used in the device A single NAND chip may be relatively slow due to its narrow asynchronous I O interface typical for SLC NAND bit asynchronous I O interface and additional high latency of basic I O operations typical for SLC NAND s to fetch a KiB page from the array to the I O buffer on a read s to commit a KiB page from the IO buffer to the array on a write ms to erase a KiB block When multiple NAND chips operate in parallel inside an SSD the bandwidth and endurance scales and the high latencies can be decreased as long as enough outstanding operations are pending and the load is evenly distributed between devices br Micron and Intel initially made faster SSDs by implementing data striping similar to RAID and interleaving in their architecture This enabled the creation of SSDs with MB s effective read write speeds with the SATA Gbit s interface in Two years later SandForce continued to leverage this parallel flash connectivity releasing consumer grade SATA Gbit s SSD controllers which supported MB s read write speeds SandForce controllers compress the data before sending it to the flash memory This process may result in less writing and higher logical throughput depending on the compressibility of the data br br br Wear leveling br br If a particular block is programmed and erased repeatedly without writing to any other blocks that block will wear out before all the other blocks thereby prematurely ending the life of the SSD For this reason SSD controllers use a technique called wear leveling to distribute writes as evenly as possible across all the flash blocks in the SSD In a perfect scenario this would enable every block to be written to its maximum life so they all fail at the same time br The process to evenly distribute writes requires data previously written and not changing cold data to be moved so that data that is changing more frequently hot data can be written into those blocks Relocating data increases write amplification and adds to the wear of flash memory so a balance must be struck between these performance considerations and wear leveling effectiveness br br br Memory br br br Flash memory br br Most SSD manufacturers use non volatile NAND flash memory in the construction of their SSDs because of the lower cost compared with DRAM and the ability to retain the data without a constant power supply ensuring data persistence through sudden power outages Flash memory SSDs were initially slower than DRAM solutions and some early designs were even slower than HDDs after continued use This problem was resolved by controllers that came out in and later br Flash based SSDs store data in metal oxide semiconductor MOS integrated circuit chips which contain non volatile floating gate memory cells Flash memory based solutions are typically packaged in standard disk drive form factors and inch but also in smaller more compact form factors such as the M form factor made possible by the small size of flash memory br Lower priced drives usually use quad level cell QLC triple level cell TLC or more rarely multi level cell MLC flash memory which is slower and less reliable than single level cell SLC flash memory This can be mitigated or even reversed by the internal design structure of the SSD such as interleaving changes to writing algorithms and higher over provisioning more excess capacity with which the wear leveling algorithms can work br Solid state drives that rely on V NAND technology in which layers of cells are stacked vertically have been introduced br br br DRAM br br SSDs based on volatile memory such as DRAM are characterized by very fast data access generally less than microseconds and are used primarily to accelerate applications that would otherwise be held back by the latency of flash SSDs or traditional HDDs br DRAM based SSDs usually incorporate either an internal battery or an external AC DC adapter and backup storage systems to ensure data persistence while no power is being supplied to the drive from external sources If power is lost the battery provides power while all information is copied from random access memory RAM to back up storage When the power is restored the information is copied back to the RAM from the back up storage and the SSD resumes normal operation similar to the hibernate function used in modern operating systems br SSDs of this type are usually fitted with DRAM modules of the same type used in regular PCs and servers which can be swapped out and replaced by larger modules br Such as i RAM HyperOs HyperDrive DDRdrive X etc br Some manufacturers of DRAM SSDs solder the DRAM chips directly to the drive and do not intend the chips to be swapped out such as ZeusRAM Aeon Drive etc br A remote indirect memory access disk RIndMA Disk uses a secondary computer with a fast network or direct Infiniband connection to act like a RAM based SSD but the new faster flash memory based SSDs already available in are making this option not as cost effective br While the price of DRAM continues to fall the price of Flash memory falls even faster br The Flash becomes cheaper than DRAM crossover point occurred approximately br br br D XPoint br In Intel and Micron announced D XPoint as a new non volatile memory technology Intel released the first D XPoint based drive branded as Intel Optane SSD in March starting with a data center product Intel Optane SSD DC P X Series and following with the client version Intel Optane SSD P Series in October Both products operate faster and with higher endurance than NAND based SSDs while the areal density is comparable at gigabits per chip For the price per bit D XPoint is more expensive than NAND but cheaper than DRAM br br br Other br br Some SSDs called NVDIMM or Hyper DIMM devices use both DRAM and flash memory When the power goes down the SSD copies all the data from its DRAM to flash when the power comes back up the SSD copies all the data from its flash to its DRAM In a somewhat similar way some SSDs use form factors and buses actually designed for DIMM modules while using only flash memory and making it appear as if it were DRAM Such SSDs are usually known as ULLtraDIMM devices br Drives known as hybrid drives or solid state hybrid drives SSHDs use a hybrid of spinning disks and flash memory Some SSDs use magnetoresistive random access memory MRAM for storing data br br br Cache or buffer br A flash based SSD may use a small amount of DRAM as a volatile cache similar to the buffers in hard disk drives A directory of block placement and wear leveling data is also kept in the cache while the drive is operating One SSD controller manufacturer SandForce does not use an external DRAM cache on their designs but still achieves high performance Such an elimination of the external DRAM reduces the power consumption and enables further size reduction of SSDs On MLC SSDs the SLC cache mechanism may be used On NVMe SSDs the Host Memory Buffer mechanism may be used br br br Battery or supercapacitor br Another component in higher performing SSDs is a capacitor or some form of battery which are necessary to maintain data integrity so the data in the cache can be flushed to the drive when power is lost some may even hold power long enough to maintain data in the cache until power is resumed In the case of MLC flash memory a problem called lower page corruption can occur when MLC flash memory loses power while programming an upper page The result is that data written previously and presumed safe can be corrupted if the memory is not supported by a supercapacitor in the event of a sudden power loss This problem does not exist with SLC flash memory br Many consumer class SSDs have built in capacitors to save at least the FTL mapping table on unexpected power loss among the examples are the Crucial M and MX series the Intel series and the more expensive Intel and series Enterprise class SSDs such as the Intel DC S series usually have built in batteries or supercapacitors br br br Host interface br br The host interface is physically a connector with the signalling managed by the SSD s controller It is most often one of the interfaces found in HDDs They include br br Serial attached SCSI SAS Gbit s generally found on servers br Serial ATA and mSATA variant SATA Gbit s br PCI Express PCIe Gbit s br M Gbit s for SATA logical device interface Gbit s for PCIe br U PCIe br Fibre Channel Gbit s almost exclusively found on servers br USB Gbit s br Parallel ATA UDMA Mbit s mostly replaced by SATA br Parallel SCSI Mbit s Mbit s generally found on servers mostly replaced by SAS last SCSI based SSD was introduced in br SSDs support various logical device interfaces such as Advanced Host Controller Interface AHCI and NVMe Logical device interfaces define the command sets used by operating systems to communicate with SSDs and host bus adapters HBAs br br br Configurations br The size and shape of any device are largely driven by the size and shape of the components used to make that device Traditional HDDs and optical drives are designed around the rotating platter s or optical disc along with the spindle motor inside Since an SSD is made up of various interconnected integrated circuits ICs and an interface connector its shape is no longer limited to the shape of rotating media drives Some solid state storage solutions come in a larger chassis that may even be a rack mount form factor with numerous SSDs inside They would all connect to a common bus inside the chassis and connect outside the box with a single connector br For general computer use the inch form factor typically found in laptops and used for most SATA ssds is the most popular For desktop computers with inch hard disk drive slots a simple adapter plate can be used to make such a drive fit Other types of form factors are more common in enterprise applications An SSD can also be completely integrated in the other circuitry of the device as in the Apple MacBook Air starting with the fall model As of mSATA and M form factors also gained popularity primarily in laptops br br br Standard HDD form factors br br The benefit of using a current HDD form factor would be to take advantage of the extensive infrastructure already in place to mount and connect the drives to the host system These traditional form factors are known by the size of the rotating media i e inch inch inch or inch and not the dimensions of the drive casing br br br Standard card form factors br br For applications where space is at a premium like for ultrabooks or tablet computers a few compact form factors were standardized for flash based SSDs br There is the mSATA form factor which uses the PCI Express Mini Card physical layout It remains electrically compatible with the PCI Express Mini Card interface specification while requiring an additional connection to the SATA host controller through the same connector br M form factor formerly known as the Next Generation Form Factor NGFF is a natural transition from the mSATA and physical layout it used to a more usable and more advanced form factor While mSATA took advantage of an existing form factor and connector M has been designed to maximize usage of the card space while minimizing the footprint The M standard allows both SATA and PCI Express SSDs to be fitted onto M modules br Some high performance high capacity drives uses standard PCI Express add in card form factor to house additional memory chips permit the use of higher power levels and allow the use of a large heat sink There are also adapter boards that converts other form factors especially M drives with PCIe interface into regular add in cards br br br Disk on a module form factors br br A disk on a module DOM is a flash drive with either pin Parallel ATA PATA or SATA interface intended to be plugged directly into the motherboard and used as a computer hard disk drive HDD DOM devices emulate a traditional hard disk drive resulting in no need for special drivers or other specific operating system support DOMs are usually used in embedded systems which are often deployed in harsh environments where mechanical HDDs would simply fail or in thin clients because of small size low power consumption and silent operation br As of storage capacities range from MB to GB with different variations in physical layouts including vertical or horizontal orientation br br br Box form factors br Many of the DRAM based solutions use a box that is often designed to fit in a rack mount system The number of DRAM components required to get sufficient capacity to store the data along with the backup power supplies requires a larger space than traditional HDD form factors br br br Bare board form factors br br br br br br br br br br br br br br br br br br br Form factors which were more common to memory modules are now being used by SSDs to take advantage of their flexibility in laying out the components Some of these include PCIe mini PCIe mini DIMM MO and many more The SATADIMM from Viking Technology uses an empty DDR DIMM slot on the motherboard to provide power to the SSD with a separate SATA connector to provide the data connection back to the computer The result is an easy to install SSD with a capacity equal to drives that typically take a full inch drive bay At least one manufacturer Innodisk has produced a drive that sits directly on the SATA connector SATADOM on the motherboard without any need for a power cable Some SSDs are based on the PCIe form factor and connect both the data interface and power through the PCIe connector to the host These drives can use either direct PCIe flash controllers or a PCIe to SATA bridge device which then connects to SATA flash controllers br br There are also SSDs that are in the form of PCIe cards these are sometimes called HHHL Half Height Half Length or AIC Add in Card SSDs br br br Ball grid array form factors br In the early s a few companies introduced SSDs in Ball Grid Array BGA form factors such as M Systems now SanDisk DiskOnChip and Silicon Storage Technology s NANDrive now produced by Greenliant Systems and Memoright s M for use in embedded systems The main benefits of BGA SSDs are their low power consumption small chip package size to fit into compact subsystems and that they can be soldered directly onto a system motherboard to reduce adverse effects from vibration and shock br Such embedded drives often adhere to the eMMC and eUFS standards br br br Comparison with other technologies br br br Hard disk drives br br Making a comparison between SSDs and ordinary spinning HDDs is difficult Traditional HDD benchmarks tend to focus on the performance characteristics that are poor with HDDs such as rotational latency and seek time As SSDs do not need to spin or seek to locate data they may prove vastly superior to HDDs in such tests However SSDs have challenges with mixed reads and writes and their performance may degrade over time SSD testing must start from the in use full drive as the new and empty fresh out of the box drive may have much better write performance than it would show after only weeks of use br Most of the advantages of solid state drives over traditional hard drives are due to their ability to access data completely electronically instead of electromechanically resulting in superior transfer speeds and mechanical ruggedness On the other hand hard disk drives offer significantly higher capacity for their price br Some field failure rates indicate that SSDs are significantly more reliable than HDDs but others do not However SSDs are uniquely sensitive to sudden power interruption resulting in aborted writes or even cases of the complete loss of the drive The reliability of both HDDs and SSDs varies greatly among models br As with HDDs there is a tradeoff between cost and performance of different SSDs Single level cell SLC SSDs while significantly more expensive than multi level MLC SSDs offer a significant speed advantage At the same time DRAM based solid state storage is currently considered the fastest and most costly with average response times of microseconds instead of the average microseconds of other SSDs Enterprise flash devices EFDs are designed to handle the demands of tier application with performance and response times similar to less expensive SSDs br In traditional HDDs a rewritten file will generally occupy the same location on the disk surface as the original file whereas in SSDs the new copy will often be written to different NAND cells for the purpose of wear leveling The wear leveling algorithms are complex and difficult to test exhaustively as a result one major cause of data loss in SSDs is firmware bugs br The following table shows a detailed overview of the advantages and disadvantages of both technologies Comparisons reflect typical characteristics and may not hold for a specific device br br br Memory cards br br While both memory cards and most SSDs use flash memory they serve very different markets and purposes Each has a number of different attributes which are optimized and adjusted to best meet the needs of particular users Some of these characteristics include power consumption performance size and reliability br SSDs were originally designed for use in a computer system The first units were intended to replace or augment hard disk drives so the operating system recognized them as a hard drive Originally solid state drives were even shaped and mounted in the computer like hard drives Later SSDs became smaller and more compact eventually developing their own unique form factors such as the M form factor The SSD was designed to be installed permanently inside a computer br In contrast memory cards such as Secure Digital SD CompactFlash CF and many others were originally designed for digital cameras and later found their way into cell phones gaming devices GPS units etc Most memory cards are physically smaller than SSDs and designed to be inserted and removed repeatedly br br br SSD failure br SSDs have very different failure modes from traditional magnetic hard drives Because solid state drives contain no moving parts they are generally not subject to mechanical failures Instead other kinds of failure are possible for example incomplete or failed writes due to sudden power failure can be more of a problem than with HDDs and if a chip fails then all the data on it is lost a scenario not applicable to magnetic drives On the whole however studies have shown that SSDs are generally highly reliable and often continue working far beyond the expected lifetime as stated by their manufacturer br The endurance of an SSD should be provided on its datasheet in one of two forms br br either n DW D n drive writes per day br or m TBW max terabytes written short TBW br So for example a Samsung EVO NVMe M SSD with TB has an endurance of TBW br br br SSD reliability and failure modes br An early investigation by Techreport com that ran from to involved a number of flash based SSDs being tested to destruction to identify how and at what point they failed The website found that all of the drives surpassed their official endurance specifications by writing hundreds of terabytes without issue volumes of that order being in excess of typical consumer needs The first SSD to fail was TLC based with the drive succeeding in writing over TB Three SSDs in the test wrote three times that amount almost PB before they too failed The test demonstrated the remarkable reliability of even consumer market SSDs br A field study based on data collected over six years in Google s data centres and spanning millions of drive days found that the proportion of flash based SSDs requiring replacement in their first four years of use ranged from to depending on the model The authors concluded that SSDs fail at a significantly lower rate than hard disk drives In contrast a evaluation of HDDs found failure rates comparable to those of Google s SSDs the HDDs had on average an annualized failure rate of The study also showed on the down side that SSDs experience significantly higher rates of uncorrectable errors which cause data loss than do HDDs It also led to some unexpected results and implications br br In the real world MLC based designs believed less reliable than SLC designs are often as reliable as SLC The findings state that SLC is not generally more reliable than MLC But generally it is said that the write endurance is the following br SLC NAND erases per block br MLC NAND to erases per block for medium capacity applications and to for high capacity applications br TLC NAND erases per block br Device age measured by days in use is the main factor in SSD reliability and not amount of data read or written which are measured by terabytes written or drive writes per day This suggests that other aging mechanisms such as silicon aging are at play The correlation is significant around br Raw bit error rates RBER grow slowly with wear out and not exponentially as is often assumed RBER is not a good predictor of other errors or SSD failure br The uncorrectable bit error rate UBER is widely used but is not a good predictor of failure either However SSD UBER rates are higher than those for HDDs so although they do not predict failure they can lead to data loss due to unreadable blocks being more common on SSDs than HDDs The conclusion states that although more reliable overall the rate of uncorrectable errors able to impact a user is larger br Bad blocks in new SSDs are common and drives with a large number of bad blocks are much more likely to lose hundreds of other blocks most likely due to Flash die or chip failure of SSDs develop at least one bad block and develop at least one bad chip in the first four years of deployment br There is no sharp increase in errors after the expected lifetime is reached br Most SSDs develop no more than a few bad blocks perhaps SSDs that develop many bad blocks often go on to develop far more perhaps hundreds and may be prone to failure However most drives are shipped with bad blocks from manufacture The finding overall was that bad blocks are common and of drives will develop at least one in use but even a few bad blocks is a predictor of up to hundreds of bad blocks at a later time The bad block count at manufacture correlates with later development of further bad blocks The report conclusion added that SSDs tended to either have less than a handful of bad blocks or a large number and suggested that this might be a basis for predicting eventual failure br Around of SSDs will develop bad chips in their first four years of use Over two thirds of these chips will have breached their manufacturers tolerances and specifications which typically guarantee that no more than of blocks on a chip will fail within its expected write lifetime br of those SSDs that need repair warranty servicing need repair only once in their life Days between repair vary from a couple of thousand days to nearly days depending on the model br br br Data recovery and secure deletion br Solid state drives have set new challenges for data recovery companies as the method of storing data is non linear and much more complex than that of hard disk drives The strategy by which the drive operates internally can vary largely between manufacturers and the TRIM command zeroes the whole range of a deleted file Wear leveling also means that the physical address of the data and the address exposed to the operating system are different br As for secure deletion of data ATA Secure Erase command could be used A program such as hdparm can be used for this purpose br br br Reliability metrics br The JEDEC Solid State Technology Association JEDEC has published standards for reliability metrics br br Unrecoverable Bit Error Ratio UBER br Terabytes Written TBW the number of terabytes that can be written to a drive within its warranty service br Drive Writes Per Day DWPD the number of times the total capacity of the drive may be written to per day within its warranty service br br br Applications br As of the cost of HDD storage was so much less that SSD was generally only used for applications where storage speed was mission critical At that time researchers predicted that cost would fall and adoption would increase More recently due to the falling price of flash memory SSD is more cost effective br In the distributed computing environment SSDs can be used as a distributed cache layer that temporarily absorbs the large volume of user requests to the slower HDD based backend storage system This layer provides much higher bandwidth and lower latency than the storage system and can be managed in a number of forms such as distributed key value database and distributed file system On supercomputers this layer is typically referred to as burst buffer With this fast layer users often experience shorter system response time br Flash based solid state drives can be used to create network appliances from general purpose personal computer hardware A write protected flash drive containing the operating system and application software can substitute for larger less reliable disk drives or CD ROMs Appliances built this way can provide an inexpensive alternative to expensive router and firewall hardware br SSDs based on an SD card with a live SD operating system are easily write locked Combined with a cloud computing environment or other writable medium to maintain persistence an OS booted from a write locked SD card is robust rugged reliable and impervious to permanent corruption If the running OS degrades simply turning the machine off and then on returns it back to its initial uncorrupted state and thus is particularly solid The SD card installed OS does not require removal of corrupted components since it was write locked though any written media may need to be restored br br br Hard drive cache br In Intel introduced a caching mechanism for their Z chipset and mobile derivatives called Smart Response Technology which allows a SATA SSD to be used as a cache configurable as write through or write back for a conventional magnetic hard disk drive A similar technology is available on HighPoint s RocketHybrid PCIe card br Solid state hybrid drives SSHDs are based on the same principle but integrate some amount of flash memory on board of a conventional drive instead of using a separate SSD The flash layer in these drives can be accessed independently from the magnetic storage by the host using ATA commands allowing the operating system to manage it For example Microsoft s ReadyDrive technology explicitly stores portions of the hibernation file in the cache of these drives when the system hibernates making the subsequent resume faster br Dual drive hybrid systems are combining the usage of separate SSD and HDD devices installed in the same computer with overall performance optimization managed by the computer user or by the computer s operating system software Examples of this type of system are bcache and dm cache on Linux and Apple s Fusion Drive br br br File system support for SSDs br br Typically the same file systems used on hard disk drives can also be used on solid state drives It is usually expected for the file system to support the TRIM command which helps the SSD to recycle discarded data support for TRIM arrived some years after SSDs themselves but is now nearly universal This means that the file system does not need to manage wear leveling or other flash memory characteristics as they are handled internally by the SSD Some log structured file systems e g F FS JFFS help to reduce write amplification on SSDs especially in situations where only very small amounts of data are changed such as when updating file system metadata br While not a native feature of file systems operating systems should also aim to align partitions correctly which avoids excessive read modify write cycles A typical practice for personal computers is to have each partition aligned to start at a MiB bytes mark which covers all common SSD page and block size scenarios as it is divisible by all commonly used sizes MiB KiB KiB KiB and B Modern operating system installation software and disk tools handle this automatically br br br Linux br Initial support for the TRIM command has been added to version of the Linux kernel mainline br The ext Btrfs XFS JFS and F FS file systems include support for the discard TRIM or UNMAP function br Kernel support for the TRIM operation was introduced in version of the Linux kernel mainline released on February To make use of it a file system must be mounted using the discard parameter Linux swap partitions are by default performing discard operations when the underlying drive supports TRIM with the possibility to turn them off or to select between one time or continuous discard operations Support for queued TRIM which is a SATA feature that results in TRIM commands not disrupting the command queues was introduced in Linux kernel released on November br An alternative to the kernel level TRIM operation is to use a user space utility called fstrim that goes through all of the unused blocks in a filesystem and dispatches TRIM commands for those areas fstrim utility is usually run by cron as a scheduled task As of November it is used by the Ubuntu Linux distribution in which it is enabled only for Intel and Samsung solid state drives for reliability reasons vendor check can be disabled by editing file etc cron weekly fstrim using instructions contained within the file itself br Since standard Linux drive utilities have taken care of appropriate partition alignment by default br br br Linux performance considerations br br During installation Linux distributions usually do not configure the installed system to use TRIM and thus the etc fstab file requires manual modifications This is because of the notion that the current Linux TRIM command implementation might not be optimal It has been proven to cause a performance degradation instead of a performance increase under certain circumstances As of January Linux sends an individual TRIM command to each sector instead of a vectorized list defining a TRIM range as recommended by the TRIM specification br For performance reasons it is recommended to switch the I O scheduler from the default CFQ Completely Fair Queuing to NOOP or Deadline CFQ was designed for traditional magnetic media and seek optimization thus many of those I O scheduling efforts are wasted when used with SSDs As part of their designs SSDs offer much bigger levels of parallelism for I O operations so it is preferable to leave scheduling decisions to their internal logic especially for high end SSDs br br A scalable block layer for high performance SSD storage known as blk multiqueue or blk mq and developed primarily by Fusion io engineers was merged into the Linux kernel mainline in kernel version released on January This leverages the performance offered by SSDs and NVMe by allowing much higher I O submission rates With this new design of the Linux kernel block layer internal queues are split into two levels per CPU and hardware submission queues thus removing bottlenecks and allowing much higher levels of I O parallelization As of version of the Linux kernel released on April VirtIO block driver the SCSI layer which is used by Serial ATA drivers device mapper framework loop device driver unsorted block images UBI driver which implements erase block management layer for flash memory devices and RBD driver which exports Ceph RADOS objects as block devices have been modified to actually use this new interface other drivers will be ported in the following releases br br br macOS br Versions since Mac OS X Snow Leopard support TRIM but only when used with an Apple purchased SSD TRIM is not automatically enabled for third party drives although it can be enabled by using third party utilities such as Trim Enabler The status of TRIM can be checked in the System Information application or in the system profiler command line tool br Versions since OS X Yosemite include sudo trimforce enable as a Terminal command that enables TRIM on non Apple SSDs There is also a technique to enable TRIM in versions earlier than Mac OS X although it remains uncertain whether TRIM is actually utilized properly in those cases br br br Microsoft Windows br Prior to version Microsoft Windows did not take any specific measures to support solid state drives From Windows the standard NTFS file system provides support for the TRIM command Other file systems on Windows do not support TRIM br By default Windows and newer versions execute TRIM commands automatically if the device is detected to be a solid state drive However because TRIM irreversibly resets all freed space it may be desirable to disable support where enabling data recovery is preferred over wear leveling To change the behavior in the Registry key HKEY LOCAL MACHINE SYSTEM CurrentControlSet Control FileSystem the value DisableDeleteNotification can be set to This prevents the mass storage driver issuing the TRIM command br Windows implements TRIM command for more than just file delete operations The TRIM operation is fully integrated with partition and volume level commands such as format and delete with file system commands relating to truncate and compression and with the System Restore also known as Volume Snapshot feature br br br Windows Vista br Windows Vista generally expects hard disk drives rather than SSDs Windows Vista includes ReadyBoost to exploit characteristics of USB connected flash devices but for SSDs it only improves the default partition alignment to prevent read modify write operations that reduce the speed of SSDs Most SSDs are typically split into KiB sectors while earlier systems may be based on byte sectors with their default partition setups unaligned to the KiB boundaries br br br Defragmentation br Defragmentation should be disabled on solid state drives because the location of the file components on an SSD does not significantly impact its performance but moving the files to make them contiguous using the Windows Defrag routine will cause unnecessary write wear on the limited number of P E cycles on the SSD The Superfetch feature will not materially improve performance and causes additional overhead in the system and SSD Windows Vista does not send the TRIM command to solid state drives but some third party utilities such as SSD Doctor will periodically scan the drive and TRIM the appropriate entries br br br Windows br Windows and later versions have native support for SSDs The operating system detects the presence of an SSD and optimizes operation accordingly For SSD devices Windows disables ReadyBoost and automatic defragmentation Despite the initial statement by Steven Sinofsky before the release of Windows however defragmentation is not disabled even though its behavior on SSDs differs One reason is the low performance of Volume Shadow Copy Service on fragmented SSDs The second reason is to avoid reaching the practical maximum number of file fragments that a volume can handle If this maximum is reached subsequent attempts to write to the drive will fail with an error message br Windows also includes support for the TRIM command to reduce garbage collection for data that the operating system has already determined is no longer valid Without support for TRIM the SSD would be unaware of this data being invalid and would unnecessarily continue to rewrite it during garbage collection causing further wear on the SSD It is beneficial to make some changes that prevent SSDs from being treated more like HDDs for example cancelling defragmentation not filling them to more than about of capacity not storing frequently written to files such as log and temporary files on them if a hard drive is available and enabling the TRIM process br br br Windows and later br Windows and later Windows systems also support automatic TRIM for PCI Express SSDs based on NVMe For Windows the KB update is required for this functionality and needs to be integrated into Windows Setup using DISM if Windows has to be installed on the NVMe SSD Windows also support the SCSI unmap command for USB attached SSDs or SATA to USB enclosures SCSI Unmap is a full analog of the SATA TRIM command It is also supported over USB Attached SCSI Protocol UASP br The graphical Windows Disk Defragmenter in Windows also recognizes SSDs distinctly from hard disk drives in a separate Media Type column While Windows supported automatic TRIM for internal SATA SSDs Windows and Windows support manual TRIM via an Optimize function in Disk Defragmenter as well as automatic TRIM for SATA NVMe and USB attached SSDs Disk Defragmenter in Windows and may execute TRIM to optimize an SSD br br br ZFS br Solaris as of version Update released in October and recent versions of OpenSolaris Solaris Express Community Edition Illumos Linux with ZFS on Linux and FreeBSD all can use SSDs as a performance booster for ZFS A low latency SSD can be used for the ZFS Intent Log ZIL where it is named the SLOG This is used every time a synchronous write to the drive occurs An SSD not necessarily with a low latency may also be used for the level Adaptive Replacement Cache L ARC which is used to cache data for reading When used either alone or in combination large increases in performance are generally seen br br br FreeBSD br ZFS for FreeBSD introduced support for TRIM on September The code builds a map of regions of data that were freed on every write the code consults the map and eventually removes ranges that were freed before but are now overwritten There is a low priority thread that TRIMs ranges when the time comes br Also the Unix File System UFS supports the TRIM command br br br Swap partitions br According to Microsoft s former Windows division president Steven Sinofsky there are few files better than the pagefile to place on an SSD According to collected telemetry data Microsoft had found the pagefile sys to be an ideal match for SSD storage However enable pagefile on SSD may increase the write amplification of SSD br Linux swap partitions are by default performing TRIM operations when the underlying block device supports TRIM with the possibility to turn them off or to select between one time or continuous TRIM operations br If an operating system does not support using TRIM on discrete swap partitions it might be possible to use swap files inside an ordinary file system instead For example OS X does not support swap partitions it only swaps to files within a file system so it can use TRIM when for example swap files are deleted br DragonFly BSD allows SSD configured swap to also be used as file system cache This can be used to boost performance on both desktop and server workloads The bcache dm cache and Flashcache projects provide a similar concept for the Linux kernel br br br Standardization organizations br The following are noted standardization organizations and bodies that work to create standards for solid state drives and other computer storage devices The table below also includes organizations which promote the use of solid state drives This is not necessarily an exhaustive list br br br Commercialization br br br Availability br Solid state drive technology has been marketed to the military and niche industrial markets since the mid s br Along with the emerging enterprise market SSDs have been appearing in ultra mobile PCs and a few lightweight laptop systems adding significantly to the price of the laptop depending on the capacity form factor and transfer speeds For low end applications a USB flash drive may be obtainable for anywhere from to or so depending on capacity and speed alternatively a CompactFlash card may be paired with a CF to IDE or CF to SATA converter at a similar cost Either of these requires that write cycle endurance issues be managed either by refraining from storing frequently written files on the drive or by using a flash file system Standard CompactFlash cards usually have write speeds of to MB s while the more expensive upmarket cards claim speeds of up to MB s br The first flash memory SSD based PC to become available was the Sony Vaio UX announced for pre order on June and began shipping in Japan on July with a GB flash memory hard drive In late September Sony upgraded the SSD in the Vaio UX to GB br One of the first mainstream releases of SSD was the XO Laptop built as part of the One Laptop Per Child project Mass production of these computers built for children in developing countries began in December These machines use MiB SLC NAND flash as primary storage which is considered more suitable for the harsher than normal conditions in which they are expected to be used Dell began shipping ultra portable laptops with SanDisk SSDs on April Asus released the Eee PC netbook on October with or gigabytes of flash memory In two manufacturers released the ultrathin laptops with SSD options instead of uncommon HDD this was a MacBook Air released by the Apple in a January with an optional GB SSD The Apple Store cost was more for this option as compared with that of an GB RPM HDD And the Lenovo ThinkPad X with a similar gigabyte SSD announced in February and upgraded to GB SSD option on August with release of ThinkPad X model an upgrade which added approximately US br In low end netbooks appeared with SSDs In SSDs began to appear in laptops br On January EMC Corporation EMC became the first enterprise storage vendor to ship flash based SSDs into its product portfolio when it announced it had selected STEC Inc s Zeus IOPS SSDs for its Symmetrix DMX systems In Sun released the Sun Storage Unified Storage Systems codenamed Amber Road which use both solid state drives and conventional hard drives to take advantage of the speed offered by SSDs and the economy and capacity offered by conventional HDDs br Dell began to offer optional GB solid state drives on select notebook models in January In May Toshiba launched a laptop with a GB SSD br Since October Apple s MacBook Air line has used a solid state drive as standard In December OCZ RevoDrive X PCIe SSD was available in GB to GB capacities delivering speeds over MB s sequential speeds and random small file writes up to IOPS In November Fusion io released its highest performing SSD drive named ioDrive Octal utilising PCI Express x Gen interface with storage space of TB read speed of GB s write speed of GB s and a low latency of microseconds It has M Read byte IOPS and M Write byte IOPS br In computers based on Intel s Ultrabook specifications became available These specifications dictate that Ultrabooks use an SSD These are consumer level devices unlike many previous flash offerings aimed at enterprise users and represent the first widely available consumer computers using SSDs aside from the MacBook Air At CES OCZ Technology demonstrated the R CloudServ PCIe SSDs capable of reaching transfer speeds of GB s and million IOPS Also announced was the Z Drive R which is available in capacities up to TB capable of reaching transfer speeds of GB s and million IOPS using the PCI Express x Gen br In December Samsung introduced and launched the industry s first TB mSATA SSD In August Samsung announced a TB SSD at the time the world s highest capacity single storage device of any type br While a number of companies offer SSD devices as of only five of the companies that offer them actually manufacture the NAND flash devices that are the storage element in SSDs br br br Quality and performance br br In general performance of any particular device can vary significantly in different operating conditions For example the number of parallel threads accessing the storage device the I O block size and the amount of free space remaining can all dramatically change the performance i e transfer rates of the device br SSD technology has been developing rapidly Most of the performance measurements used on disk drives with rotating media are also used on SSDs Performance of flash based SSDs is difficult to benchmark because of the wide range of possible conditions In a test performed in by Xssist using IOmeter KB random read write queue depth the IOPS delivered by the Intel X E GB G started around IOPs and dropped sharply after minutes to IOPS and continued to decrease gradually for the next minutes IOPS vary between and from around minutes onwards for the rest of the hour test run br Designers of enterprise grade flash drives try to extend longevity by increasing over provisioning and by employing wear leveling br br br Sales br br SSD shipments were million units in million units in for a total of US billion million units in and were expected to rise to million units in br to million units in and to million units in br Revenues for the SSD market including low cost PC solutions worldwide totaled million in rising over from million in br br br See also br Board solid state drive br List of solid state drive manufacturers br List of flash memory controller manufacturers br Hard disk drive br RAID br Flash Core Module br RAM drive br br br br br br Further reading br Solid state revolution in depth on how SSDs really work Lee Hutchinson Ars Technica June br Mai Zheng Joseph Tucek Feng Qin Mark Lillibridge Understanding the Robustness of SSDs under Power Fault FAST br Cheng Li Philip Shilane Fred Douglis Hyong Shim Stephen Smaldone Grant Wallace Nitro A Capacity Optimized SSD Cache for Primary Storage USENIX ATC br br br External links br br br Background and general br Understanding SSDs and New Drives from OCZ br Charting the Year Rise of the Solid State Disk Market br Investigation Is Your SSD More Reliable Than A Hard Drive long term SSD reliability review br br br Other br JEDEC Continues SSD Standardization Efforts br Linux NVM File and Storage System Challenges PDF br Linux and SSD Optimization br Understanding the Robustness of SSDs under Power Fault USENIX by Mai Zheng Joseph Tucek Feng Qin and Mark Lillibridge 