title: NVM Express
id: 31093533
NVM Express NVMe or Non Volatile Memory Host Controller Interface Specification NVMHCIS is an open logical device interface specification for accessing a computer s non volatile storage media usually attached via the PCI Express bus The initial NVM stands for non volatile memory which is often NAND flash memory that comes in several physical form factors including solid state drives SSDs PCIe add in cards and M cards the successor to mSATA cards NVM Express as a logical device interface has been designed to capitalize on the low latency and internal parallelism of solid state storage devices br Architecturally the logic for NVMe is physically stored within and executed by the NVMe controller chip that is physically co located with the storage media usually an SSD Version changes for NVMe e g to are incorporated within the storage media and do not affect PCIe compatible components such as motherboards and CPUs br By its design NVM Express allows host hardware and software to fully exploit the levels of parallelism possible in modern SSDs As a result NVM Express reduces I O overhead and brings various performance improvements relative to previous logical device interfaces including multiple long command queues and reduced latency The previous interface protocols like AHCI were developed for use with far slower hard disk drives HDD where a very lengthy delay relative to CPU operations exists between a request and data transfer where data speeds are much slower than RAM speeds and where disk rotation and seek time give rise to further optimization requirements br NVM Express devices are chiefly available in the form of standard sized PCI Express expansion cards and as inch form factor devices that provide a four lane PCI Express interface through the U connector formerly known as SFF Storage devices using SATA Express and the M specification which support NVM Express as the logical device interface are a popular use case for NVMe and have become the dominant form of solid state storage for servers desktops and laptops alike br br br Specifications br Specifications for NVMe released to date include br br e January br b July br November br a October br b June br June br May br a October br b May br c May br d March br June br a March br b September br c June br May br a July br b January br c October br d January br br br Background br br Historically most SSDs used buses such as SATA SAS or Fibre Channel for interfacing with the rest of a computer system Since SSDs became available in mass markets SATA has become the most typical way for connecting SSDs in personal computers however SATA was designed primarily for interfacing with mechanical hard disk drives HDDs and it became increasingly inadequate for SSDs which improved in speed over time For example within about five years of mass market mainstream adoption many SSDs were already held back by the comparatively slow data rates available for hard drives unlike hard disk drives some SSDs are limited by the maximum throughput of SATA br High end SSDs had been made using the PCI Express bus before NVMe but using non standard specification interfaces By standardizing the interface of SSDs operating systems only need one common device driver to work with all SSDs adhering to the specification It also means that each SSD manufacturer does not have to design specific interface drivers This is similar to how USB mass storage devices are built to follow the USB mass storage device class specification and work with all computers with no per device drivers needed br NVM Express devices are also used as the building block of the burst buffer storage in many leading supercomputers such as Fugaku Supercomputer Summit Supercomputer and Sierra Supercomputer etc br br br History br The first details of a new standard for accessing non volatile memory emerged at the Intel Developer Forum when NVMHCI was shown as the host side protocol of a proposed architectural design that had Open NAND Flash Interface Working Group ONFI on the memory flash chips side A NVMHCI working group led by Intel was formed that year The NVMHCI specification was completed in April and released on Intel s web site br Technical work on NVMe began in the second half of The NVMe specifications were developed by the NVM Express Workgroup which consists of more than companies Amber Huffman of Intel was the working group s chair Version of the specification was released on March while version of the specification was released on October Major features added in version are multi path I O with namespace sharing and arbitrary length scatter gather I O It is expected that future revisions will significantly enhance namespace management Because of its feature focus NVMe was initially called Enterprise NVMHCI An update for the base NVMe specification called version e was released in January In June a Promoter Group led by seven companies was formed br The first commercially available NVMe chipsets were released by Integrated Device Technology HF P AG and HF P AG in August The first NVMe drive Samsung s XS enterprise drive was announced in July according to Samsung this drive supported GB s read speeds six times faster than their previous enterprise offerings The LSI SandForce SF controller family released in November also supports NVMe A Kingston HyperX prosumer product using this controller was showcased at the Consumer Electronics Show and promised similar performance In June Intel announced their first NVM Express products the Intel SSD data center family that interfaces with the host through PCI Express bus which includes the DC P series the DC P series and the DC P series As of November NVMe drives are commercially available br In March the group incorporated to become NVM Express Inc which as of November consists of more than companies from across the industry NVM Express specifications are owned and maintained by NVM Express Inc which also promotes industry awareness of NVM Express as an industry wide standard NVM Express Inc is directed by a thirteen member board of directors selected from the Promoter Group which includes Cisco Dell EMC HGST Intel Micron Microsoft NetApp Oracle PMC Samsung SanDisk and Seagate br In September the CompactFlash Association announced that it would be releasing a new memory card specification CFexpress which uses NVMe br NVMe Host Memory Buffer HMB added in version of the NVMe specification HMB allows SSDs to utilize the host s DRAM which can improve the I O performance for DRAM less SSDs For example HMB can be used for cache the FTL table by the SSD controller which can improve I O performance NVMe added Zoned Namespaces ZNS and support for rotating media such as hard drives ZNS allows data to be mapped directly to its physical location in flash memory to directly access data on an SSD without a flash translation layer br br br Form factors br There are many form factors of NVMe solid state drive such as AIC U U M etc br br br AIC add in card br Almost all early NVMe solid state drives are HHHL half height half length or FHHL full height half length AIC with a PCIe or interface A HHHL NVMe solid state drive card is easy to insert into a PCIe slot of a server br br br U SFF br br U formerly known as SFF is a computer interface for connecting solid state drives to a computer It uses up to four PCI Express lanes Available servers can combine up to U NVMe solid state drives br br br U SFF or SFF TA br U is built on the U spec and uses the same SFF connector It is a tri mode standard combining SAS SATA and NVMe support into a single controller U can also support hot swap between the different drives where firmware support is available U drives are still backward compatible with U but U drives are not compatible with U hosts br br br M br br M formerly known as the Next Generation Form Factor NGFF uses a M NVMe solid state drive computer bus Interfaces provided through the M connector are PCI Express or higher up to four lanes br br br EDSFF br br br NVMe oF br NVM Express over Fabrics NVMe oF is the concept of using a transport protocol over a network to connect remote NVMe devices contrary to regular NVMe where physical NVMe devices are connected to a PCIe bus either directly or over a PCIe switch to a PCIe bus In August a standard for using NVMe over Fibre Channel FC was submitted by the standards organization International Committee for Information Technology Standards ICITS and this combination is often referred to as FC NVMe or sometimes NVMe FC br As of May supported NVMe transport protocols are br br FC FC NVMe br TCP NVMe TCP br Ethernet RoCE v v RDMA over converged Ethernet br InfiniBand NVMe over InfiniBand or NVMe IB br The standard for NVMe over Fabrics was published by NVM Express Inc in br The following software implements the NVMe oF protocol br br Linux NVMe oF initiator and target RoCE transport was supported initially and with Linux kernel x native support for TCP was added br Storage Performance Development Kit SPDK NVMe oF initiator and target drivers Both RoCE and TCP transports are supported br StarWind NVMe oF initiator and target for Linux and Microsoft Windows supporting both RoCE TCP and Fibre Channel transports br Lightbits Labs NVMe over TCP target for various Linux distributions public clouds br Bloombase StoreSafe Intelligent Storage Firewall supports NVMe over RoCE TCP and Fibre Channel for transparent storage security protection br br br Comparison with AHCI br The Advanced Host Controller Interface AHCI has the benefit of wide software compatibility but has the downside of not delivering optimal performance when used with SSDs connected via the PCI Express bus As a logical device interface AHCI was developed when the purpose of a host bus adapter HBA in a system was to connect the CPU memory subsystem with a much slower storage subsystem based on rotating magnetic media As a result AHCI introduces certain inefficiencies when used with SSD devices which behave much more like RAM than like spinning media br The NVMe device interface has been designed from the ground up capitalizing on the lower latency and parallelism of PCI Express SSDs and complementing the parallelism of contemporary CPUs platforms and applications At a high level the basic advantages of NVMe over AHCI relate to its ability to exploit parallelism in host hardware and software manifested by the differences in command queue depths efficiency of interrupt processing the number of uncacheable register accesses etc resulting in various performance improvements br The table below summarizes high level differences between the NVMe and AHCI logical device interfaces br br br Operating system support br br ChromeOS br On February support for booting from NVM Express devices was added to ChromeOS br DragonFly BSD br The first release of DragonFly BSD with NVMe support is version br FreeBSD br Intel sponsored a NVM Express driver for FreeBSD s head and stable branches The nvd and nvme drivers are included in the GENERIC kernel configuration by default since FreeBSD version in br Genode br Support for consumer grade NVMe was added to the Genode framework as part of the release br Haiku br Haiku gained support for NVMe on April br illumos br illumos received support for NVMe on October br iOS br With the release of the iPhone S and S Plus Apple introduced the first mobile deployment of NVMe over PCIe in smartphones Apple followed these releases with the release of the first generation iPad Pro and first generation iPhone SE that also use NVMe over PCIe br Linux br Intel published an NVM Express driver for Linux on March which was merged into the Linux kernel mainline on January and released as part of version of the Linux kernel on March Linux supports NVMe Host Memory Buffer from version with default maximum size MB br macOS br Apple introduced software support for NVM Express in Yosemite The NVMe hardware interface was introduced in the MacBook and MacBook Pro br NetBSD br NetBSD added support for NVMe in NetBSD The implementation is derived from OpenBSD br OpenBSD br Development work required to support NVMe in OpenBSD has been started in April by a senior developer formerly responsible for USB and AHCI support Support for NVMe has been enabled in the OpenBSD release br OS br Arca Noae provides an NVMe driver for ArcaOS as of April The driver requires advanced interrupts as provided by the ACPI PSD running in advanced interrupt mode mode thus requiring the SMP kernel as well br Solaris br Solaris received support for NVMe in Oracle Solaris br VMware br Intel has provided an NVMe driver for VMware which is included in vSphere and later builds supporting various NVMe devices As of vSphere update VMware s VSAN software defined storage subsystem also supports NVMe devices br Windows br Microsoft added native support for NVMe to Windows and Windows Server R Native drivers for Windows and Windows Server R have been added in updates Many vendors have released their own Windows drivers for their devices as well There are also manually customized installer files available to install a specific vendor s driver to any NVMe card such as using a Samsung NVMe driver with a non Samsung NVMe device which may be needed for additional features performance and stability Support for NVMe HMB was added in Windows Anniversary Update Version in br Support for NVMe ZNS was added in Windows Version in The OpenFabrics Alliance maintains an open source NVMe Windows Driver for Windows and Windows Server R R developed from the baseline code submitted by several promoter companies in the NVMe workgroup specifically IDT Intel and LSI The current release is from December br br br Software support br QEMU br NVMe is supported by QEMU since version released on August NVMe devices presented to QEMU guests can be either real or emulated br UEFI br An open source NVMe driver for UEFI is available on SourceForge br br br Management tools br br br nvmecontrol br The nvmecontrol tool is used to control an NVMe disk from the command line on FreeBSD It was added in FreeBSD br br br nvme cli br NVM Express user space tooling for Linux br br br See also br M br PCI Express br SATA Express br Solid state drive br Universal Flash Storage UFS br br br br br br External links br br Official website br NVMe info br CompactFlash Association br LFCS Preparing Linux for nonvolatile memory devices LWN net April by Jonathan Corbet br Multipathing PCI Express Storage Linux Foundation March by Keith Busch br NVMe NVMe oF and RDMA for network engineers August by Jerome Tissieres